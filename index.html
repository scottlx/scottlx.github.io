<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,minimum-scale=1,maximum-scale=1"><link href=/css/fonts.css rel=stylesheet type=text/css><title>windseek</title><link rel=stylesheet href=/css/hugo-octopress.css><link rel=stylesheet href=/css/fork-awesome.min.css><link href=https://scottlx.github.io/favicon.png rel=icon><link href=/index.xml rel=alternate type=application/rss+xml title=windseek><meta name=description content><meta name=keywords content><meta name=author content="windseek"><meta name=generator content="Hugo 0.119.0"></head><body><header role=banner><hgroup><h1><a href=https://scottlx.github.io/>windseek</a></h1><h2>be curious, be free</h2></hgroup></header><nav role=navigation><fieldset class=mobile-nav><select onchange="location=this.value"><option value>Navigate…</option><option value=https://scottlx.github.io/>» Blog</option><option value=https://scottlx.github.io/archives/>» Archives</option><option value=https://scottlx.github.io/cheatsheet/>» CheatSheet</option><option value=https://scottlx.github.io/about/>» Aboutme</option></select></fieldset><ul class=main-navigation><li><a href=https://scottlx.github.io/ title=Blog>Blog</a></li><li><a href=https://scottlx.github.io/archives/ title=Archives target=_blank rel="noopener noreferrer">Archives</a></li><li><a href=https://scottlx.github.io/cheatsheet/ title=CheatSheet target=_blank rel="noopener noreferrer">CheatSheet</a></li><li><a href=https://scottlx.github.io/about/ title=Aboutme target=_blank rel="noopener noreferrer">Aboutme</a></li></ul><ul class=subscription></ul><form action=https://www.google.com/search method=get target=_blank rel="noopener noreferrer"><fieldset role=search><input class=search type=text name=q results=0 placeholder=Search>
<input type=hidden name=q value=site:https://scottlx.github.io/></fieldset></form></nav><div id=main><div id=content><div class=blog-index><article><header><p class=meta>May 30, 2025
- 2 minute read
- <a href=https://scottlx.github.io/posts/bpftrace/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/bpftrace/>bpftrace注入kfunc</a></h1></header><p>四种探针类型 kprobe 定义：动态内核探针，允许在任意内核函数的入口（kprobe）或退出（kretprobe）插入断点。 特点： 动态性：无需修改内核代码，运行时动态注入。 灵活性：可跟踪几乎所有内核函数（包括未导出的符号）。 开销：较高（需修改指令、处理陷阱），可能影响性能。 稳定性：内核函数可能随版本变化，导致跟踪点失效。 用途：调试、性能分析、动态跟踪未预设的事件。 kfunc 定义：eBPF程序可调用的内核函数，由内核显式导出供安全调用。 特点： 安全性：仅允许调用内核标记为BTF_ID的特定函数（通过BPF Type Format, BTF）。 性能：直接调用内核函数，比eBPF Helper更高效。 依赖eBPF：需通过eBPF验证器确保安全性。 用途：允许eBPF程序安全访问内核内部数据结构或功能（如操作链表、修改特定字段）。 tracepoint 定义：内核静态跟踪点，由开发者预置在代码中的稳定事件接口。 特点： 静态性：需内核开发者预先定义，位置和参数格式固定。 稳定性：接口向后兼容，适合生产环境。 结构化数据：参数以明确结构体传递（如trace_sched_switch）。 低开销：相比kprobe，性能影响较小。 用途：监控系统调用、调度事件等预定义内核事件。 ** rawtracepoint** 定义：直接访问tracepoint的原始参数，跳过内核封装层。 特点： 底层访问：直接读取寄存器或原始参数，无需解析结构体。 性能优势：比常规tracepoint更高效（减少封装开销）。 不稳定性：参数格式可能随内核变化，需手动适配。 依赖eBPF：常用于eBPF程序（如BPF_PROG_TYPE_RAW_TRACEPOINT类型）。 用途：需要极致性能的场景（如高频事件跟踪），同时接受潜在兼容性风险。 btf支持 BPF 类型格式 (BTF) — Linux 内核文档 - Linux 内核
系统若有btf支持，结构体格式就可以通过btf传给工具，用户无需去查阅内核源码
sudo bpftrace --info |& grep -i btf kfunc probe 调试ebpf代码时，遇到icmp request接收了，但是内核协议栈不返回icmp reply
下面给出上述问题的排查思路
查看/proc/net/snmp发现有InHdrErrors
或者执行
netstat -s 发现Ip协议栈有invalid headers的报错
查看icmp_开头的kfunc点
bpftrace -lv 'kfunc:icmp_*' 输出</p><footer><a href=https://scottlx.github.io/posts/bpftrace/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>May 30, 2025
- 1 minute read
- <a href=https://scottlx.github.io/posts/cilium-endpoint-%E5%88%9B%E5%BB%BA%E6%B5%81%E7%A8%8B/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/cilium-endpoint-%E5%88%9B%E5%BB%BA%E6%B5%81%E7%A8%8B/>cilium endpoint 创建流程</a></h1></header><p>pod创建后，cilium打通网络涉及以下内容：
lxc网卡的创建（cni插件） bpf代码的加载（agent） ipam 地址分配（agent） endpoint CR的创建（agent） endpoint生命周期管理（agent） 具体流程如下图
流程说明
cni add流程主要分三步：
调用ipam接口从agent获取ip信息 创建lxc网卡（veth），根据ip信息配置网卡（mtu，gso，gro配置），容器命名空间内路由等 调用endpointCreate接口通知agent开始接管lxc网卡（加载bpf代码等） 如果启用了cniChaning，还会去执行chaining的动作
ipam流程将在后期详细介绍，本篇主要分析endpointCreate之后的流程，也就是bpf代码是如何加载到lxc网卡上的。
cilium controller cilium agent代码内部，对于资源同步的场景，设计了一套controller框架。
controller可以理解为异步任务控制器，在后台尝试某一对象的同步任务直到成功，并记录成功失败次数，错误日志等监控数据。每个controller对应一个协程。
controller需要被manager绑定，而manager则绑定到某一特定资源，比如endpoint
由于资源的变配会需要多个异步任务的执行，因此一个manager可以关联多个controller，单个controller只负责某一特定的异步任务(只要是可能失败并需要重试的任务都适用，例如给k8s资源打annotation，同步对象到某个存储，kvstore，bpfmap等）
controller之间通信通过eventqueue进行异步解耦。每个evq对应一个协程
endpoint manager的架构图 说明
上图中，endpoint manager维护了本节点endpoint列表，并实现了
gc controller：endpoint定期清理，清理不健康的endpoint。 regenerate controller：endpoint定期全量重建，重建ep对应的policy和configuration CNI创建endpoint后，endpoint对象会被创建。每个endpoint初始化时会有一个eventQueue和处理该eventQueue的一个controller。endpoint manager会将regen事件入队到endpoint的eventQueue中，并启动endpoint的sync Controller。sync Controller会同步ep信息到k8s cep CR，这样用户就可以从apiserver获取endpoint状态了
ebpf程序加载流程 eventQueue中的regeneration event会触发endpoint的重建，也就是相关ebpf程序的编译加载和ebpf map数据的插入。
bpf程序加载会发生在几种情况下：
第一次创建时进行初始化 cilium重启时，会进行一次regenerate（按需初始化） 用户执行cilium endpoint regenerate 时（按需） 用户执行ciluim config时（按需） 程序是否加载由编译级别来控制
0 -> “invalid” (未设置)
1：RegenerateWithoutDatapath -> “no-rebuild” （更新policy，dns，只需更新map，不重新编译加载bpf）
2：RegenerateWithDatapath -> &ldquo;rewrite+load&rdquo; （新创建endpoint）
这边可能会有个疑问，为什么更新policy不是更新lxc代码。这是因为policy代码是在bpf_lxc的最后使用尾调用执行的。因此更新policy只要更新prog bpf，而不需要动已经加载在lxc上的代码
static __always_inline int l3_local_delivery(struct __ctx_buff *ctx, __u32 seclabel, __u32 magic __maybe_unused, const struct endpoint_info *ep __maybe_unused, __u8 direction __maybe_unused, bool from_host __maybe_unused, bool from_tunnel __maybe_unused, __u32 cluster_id __maybe_unused) { /*省略一些代码*/ /* Jumps to destination pod's BPF program to enforce ingress policies.</p><footer><a href=https://scottlx.github.io/posts/cilium-endpoint-%E5%88%9B%E5%BB%BA%E6%B5%81%E7%A8%8B/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>May 27, 2025
- 14 minute read
- <a href=https://scottlx.github.io/posts/dpdk%E8%BD%AC%E5%8F%91%E9%9D%A2trace/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/dpdk%E8%BD%AC%E5%8F%91%E9%9D%A2trace/>dpdk转发面trace</a></h1></header><p>上一期分析了ebpf转发面通过linux perf event的思路进行trace，这一期介绍一种dpdk程序的trace方法。基本原理大致相通，也是通过共享内存的方式进行数据传输。转发面代码和工具代码通过mmap共享一段内存，转发面产生数据，工具代码消费数据。
共享内存buffer 由于大部分dpdk程序是run to completion模型，一个lcore对应一个网卡队列，报文尽可能不要在cpu之间来回切换，而是由单个cpu处理完所有逻辑之后发送。因此，相对应的，我们的buffer要为每一个cpu都分配一个ring
extern struct trace_buffer *g_trace; typedef OBJ_RING(struct trace_record, 1 &lt;&lt; 11) __rte_cache_aligned trace_ring_t; struct trace_buffer { trace_ring_t percpu[16]; int enabled; uint8_t ports[MAX_PORT][NR_TRACE_ON]; }; ring buffer 常规做法是使用dpdk lib的rte_ring，但是该ring的实现比较复杂，且很多功能我们不会用到。因此下面介绍一个简易的ring实现，相比dpdk rte_ring，内存占用较少，逻辑简单。
核心思路
使用内存屏障rte_smp_wmb()/rte_smp_rmb()替代锁，保证线程安全 使用prepare，commit两段式提交，确保数据一致性 使用mask位运算代替取模获取索引，提升查找效率 具体实现代码如下
#define OBJ_RING(obj_type, ring_size) \ struct { \ uint64_t next_seq; \ obj_type buffer[IS_POWER_OF_2(ring_size) ? \ (int)((ring_size) + MEMBER_TYPE_ASSERT(obj_type, seq, uint64_t)) : -1]; \ } #define obj_ring_type(ring) typeof((ring)->buffer[0]) #define obj_ring_mask(ring) (ARRAY_SIZE((ring)->buffer) - 1) #define obj_ring_at(ring, seq) (&(ring)->buffer[(seq) & obj_ring_mask(ring)]) #define obj_ring_init(ring) do { \ obj_ring_type(ring) *_obj; \ (ring)->next_seq = 0; \ array_for_each(_obj, (ring)->buffer) \ _obj->seq = 0; \ } while (0) #define obj_ring_write_prepare(ring) ({ \ obj_ring_type(ring) *_obj = obj_ring_at((ring), (ring)->next_seq); \ _obj->seq = (ring)->next_seq; \ rte_smp_wmb(); \ _obj; \ }) #define obj_ring_write_commit(ring) do { \ rte_smp_wmb(); \ (ring)->next_seq++; \ } while (0) #define obj_ring_next_seq(ring) ACCESS_ONCE((ring)->next_seq) #define obj_ring_read(ring, nseq, res) ({ \ uint64_t _seq = (nseq); \ obj_ring_type(ring) *_res = NULL; \ if (_seq &lt; obj_ring_next_seq(ring)) { \ obj_ring_type(ring) *_obj = obj_ring_at((ring), _seq); \ _res = (res); \ do { \ _seq = ACCESS_ONCE(_obj->seq); \ while (_seq >= obj_ring_next_seq(ring)) {} \ rte_smp_rmb(); \ *_res = ACCESS_ONCE(*_obj); \ rte_smp_rmb(); \ _res->seq = ACCESS_ONCE(_obj->seq); \ } while (_seq !</p><footer><a href=https://scottlx.github.io/posts/dpdk%E8%BD%AC%E5%8F%91%E9%9D%A2trace/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>May 26, 2025
- 10 minute read
- <a href=https://scottlx.github.io/posts/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90cilium-monitor%E6%9C%BA%E5%88%B6/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90cilium-monitor%E6%9C%BA%E5%88%B6/>深入剖析cilium monitor机制</a></h1></header><p>可调试性 报文转发面组件中，可调试性十分关键。开发阶段可能可以使用gdb（ebpf甚至不能用gdb，只能用trace_printk），log等方式进行调试，但到了生产环境，以下几个功能是必须要完备的：
抓包手段
按照网卡抓包 按照流进行抓包 按照特定过滤条件抓包，例如源目的地址，端口，协议号等 报文计数
收发包计数：rx，tx阶段计数 丢包计数：按照错误码进行区分 特定观测点计数：一些重要转发函数，例如l3_fwd, arp_response等 流日志
流量方向：egress/ingress session信息：五元组，nat信息，tcp状态等 其他必要的上下文：例如转发表项查找的结果，构造的action，硬件卸载标记等 linux perf_events ebpf perf基于linux perf_event子系统。epbf通知用户态拷贝数据时基于perf_events的
perf buffer ebpf中提供了内核和用户空间之间高效地交换数据的机制：perf buffer。它是一种per-cpu的环形缓冲区，当我们需要将ebpf收集到的数据发送到用户空间记录或者处理时，就可以用perf buffer来完成。它还有如下特点：
能够记录可变长度数据记； 能够通过内存映射的方式在用户态读取读取数据，而无需通过系统调用陷入到内核去拷贝数据； 实现epoll通知机制 因此在cilium中，实现上述调试手段的思路，就是在转发面代码中构造相应的event到EVENTS_MAP，之后通过别的工具去读取并解析EVENTS_MAP中的数据
EVENTS_MAP定义如下: bpf/lib/events.h
struct { __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY); __uint(key_size, sizeof(__u32)); __uint(value_size, sizeof(__u32)); __uint(pinning, LIBBPF_PIN_BY_NAME); __uint(max_entries, __NR_CPUS__); } EVENTS_MAP __section_maps_btf; key是cpu的编号，因此大小是u32；value一般是文件描述符fd，关联一个perf event，因此也是u32
数据面代码构造好data之后，使用helper function: bpf_perf_event_output通知用户态代码拷贝数据
下面是cilium代码中封装好的event输出函数，最终就是调用的bpf_perf_event_output
// bpf/include/bpf/ctx/skb.h #define ctx_event_output skb_event_output // bpf/include/bpf/helpers_skb.h /* Events for user space */ static int BPF_FUNC_REMAP(skb_event_output, struct __sk_buff *skb, void *map, __u64 index, const void *data, __u32 size) = (void *)BPF_FUNC_perf_event_output; //对应的func id 是 25 // /usr/include/linux/bpf.</p><footer><a href=https://scottlx.github.io/posts/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90cilium-monitor%E6%9C%BA%E5%88%B6/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>Mar 6, 2025
- 11 minute read
- <a href=https://scottlx.github.io/posts/bpf_lpm_trie/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/bpf_lpm_trie/>bpf lpm trie</a></h1></header><p>lpm有多种实现方式，最常用的是用trie。当然也会有更简单的实现方式，例如某些特定场景用多重哈希表就能解决（ipv4地址，32个掩码对应32个哈希表）
从4.11内核版本开始，bpf map引入了BPF_MAP_TYPE_LPM_TRIE
主要是用于匹配ip地址，内部是将数据存储在一个不平衡的trie中，key使用prefixlen,data
data是以大端网络序存储的，data[0]存的是msb。
prefixlen支持8的整数倍，最高可以是2048。因此除了ip匹配，还可以用来做端口，协议，vpcid等等的扩充匹配。在应用层面上除了做路由表，还可以作为acl，policy等匹配过滤的底层实现
使用方式 BPF_MAP_TYPE_LPM_TRIE — The Linux Kernel documentation
除了上述基本的Ipv4的使用方式，扩展使用方式可以参考一下cillium中IPCACHE_MAP的使用
首先是map的定义
struct ipcache_key { struct bpf_lpm_trie_key lpm_key; __u16 cluster_id; __u8 pad1; __u8 family; union { struct { __u32 ip4; __u32 pad4; __u32 pad5; __u32 pad6; }; union v6addr ip6; }; } __packed; /* Global IP -> Identity map for applying egress label-based policy */ struct { __uint(type, BPF_MAP_TYPE_LPM_TRIE); __type(key, struct ipcache_key); __type(value, struct remote_endpoint_info); __uint(pinning, LIBBPF_PIN_BY_NAME); __uint(max_entries, IPCACHE_MAP_SIZE); __uint(map_flags, BPF_F_NO_PREALLOC); } IPCACHE_MAP __section_maps_btf; 可以看到cillium将v4和v6合并成一个map查询，匹配条件并带上了cluster_id</p><footer><a href=https://scottlx.github.io/posts/bpf_lpm_trie/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>Mar 3, 2025
- 5 minute read
- <a href=https://scottlx.github.io/posts/cilium-datapath/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/cilium-datapath/>cilium datapath</a></h1></header><p>hook点 大部分是挂载位置是tc，tc是网络协议栈初始处理挂载点
// linux source code: dev.c __netif_receive_skb_core | list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {...} // packet capture | do_xdp_generic // handle generic xdp | sch_handle_ingress // tc ingress | tcf_classify | __tcf_classify // ebpf program is working here 如果没有下发policy，xdp就不会挂载各类filter程序
网络设备 cillium的网络方案不像常规的网桥模式（ovs，linux bridge），datapath不是一个完整的run to completion，而是分散在各个虚拟接口上，类似pipeline模式
cillium_host: 集群内所有podCIDR的网关，地址对容器可见
cilium_net: cilium_host的veth对，ipvlan模式才会用到？
clilium_vxlan: 用来提供Pod跨节点通信overlay封装
lxcXXXX: 容器veth对在主机侧的接口
同节点pod2pod cillium_host是所有pod的网关，因此会先arp request该地址。arp相应其实是在lxc处被代答了，arp报文不会走到cillium_host
// bpf_lxc.c __section_entry int cil_from_container(struct __ctx_buff *ctx) { ... case bpf_htons(ETH_P_ARP): ret = tail_call_internal(ctx, CILIUM_CALL_ARP, &amp;ext_err); break; .</p><footer><a href=https://scottlx.github.io/posts/cilium-datapath/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>Feb 18, 2025
- 1 minute read
- <a href=https://scottlx.github.io/posts/dpvs-icmp/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/dpvs-icmp/>dpvs icmp session</a></h1></header><p>原生的ipvs仅处理三种类型的ICMP报文：ICMP_DEST_UNREACH、ICMP_SOURCE_QUENCH和ICMP_TIME_EXCEEDED
对于不是这三种类型的ICMP，则设置为不相关联(related)的ICMP，返回NF_ACCEPT，之后走本机路由流程
dpvs对ipvs进行了一些修改，修改后逻辑如下
icmp差错报文流程 __dp_vs_in
__dp_vs_in_icmp4 （处理icmp差错报文，入参related表示找到了关联的conn）
若不是ICMP_DEST_UNREACH，ICMP_SOURCE_QUENCH，ICMP_TIME_EXCEEDED，返回到_dp_vs_in走普通conn命中流程
icmp差错报文，需要将报文头偏移到icmp头内部的ip头，根据内部ip头查找内部ip的conn。
若找到conn，表明此ICMP报文是由之前客户端的请求报文所触发的，由真实服务器回复的ICMP报文。将related置1
若未找到则返回accept，返回到_dp_vs_in走普通conn命中流程
​ __xmit_inbound_icmp4 ​ 找net和local路由，之后走__dp_vs_xmit_icmp4
__dp_vs_xmit_icmp4 ​ 数据区的前8个字节恰好覆盖了TCP报文或UDP报文中的端口号字段（前四个字节）
inbound方向根据内部ip的conn修改数据区目的端口为conn->dport，源端口改为conn->localport，
outbound方向将目的端口改为conn->cport，源端口改为conn->vport
​
client (cport ) &lt;&ndash;> (vport)lb(lport) &lt;&ndash;> rs(dport)
​ 重新计算icmp头的checksum，走ipv4_output
实际应用上的问题
某个rs突然下线，导致有时访问vip轮询到了不可达的rs，rs侧的网关发送了一个dest_unreach的icmp包
该rs的conn还未老化，__dp_vs_in_icmp4流程根据这个icmp的内部差错ip头找到了还未老化的conn，将icmp数据区的port进行修改发回给client
但是一般情况，rs下线后，该rs的conn会老化消失，内层conn未命中，还是走外层icmp的conn命中流程转给client。这样内部数据区的端口信息是错的（dport->lport，正确情况是vport->cport）
非差错报文流程 返回_dp_vs_in走普通conn命中流程
原本dp_vs_conn_new流程中，先查找svc。icmp的svc默认使用端口0进行查找。但是ipvsadm命令却对端口0的service添加做了限制，导致无法添加这类svc。
svc = dp_vs_service_lookup(iph->af, iph->proto, &amp;iph->daddr, 0, 0, mbuf, NULL, &amp;outwall, rte_lcore_id()); 若未查到走INET_ACCEPT(也就是继续往下进行走到ipv4_output_fin2查到local路由，若使用dpip addr配上了vip或lip地址，则会触发本地代答)。
若查到svc，则进行conn的schedule，之后会走dp_vs_laddr_bind，但是dp_vs_laddr_bind不支持icmp协议(可以整改)，最终导致svc可以查到但是conn无法建立，最后走INET_DROP。
概括一下：
未命中svc，走后续local route，最终本地代答 命中svc后若conn无法建立，drop 命中svc且建立conn，发往rs或client icmp的conn ​
_ports[0] = icmp4_id(ich); _ports[1] = ich->type &lt;&lt; 8 | ich->code; Inbound hash和outboundhash的五元组都使用上述这两个port进行哈希，并与conn进行关联。</p><footer><a href=https://scottlx.github.io/posts/dpvs-icmp/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>Feb 18, 2025
- 1 minute read
- <a href=https://scottlx.github.io/posts/dpvs-route%E8%BD%AC%E5%8F%91/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/dpvs-route%E8%BD%AC%E5%8F%91/>dpvs route转发</a></h1></header><p>ipv4_rcv_fin 是路由转发逻辑， INET_HOOKPRE_ROUTING 中走完hook逻辑后，根据dpvs返回值走ipv4_rcv_fin
路由表结构体 struct route_entry { uint8_t netmask; short metric; uint32_t flag; unsigned long mtu; struct list_head list; struct in_addr dest; //cf->dst.in struct in_addr gw;// 下一跳地址，0说明是直连路由，下一跳地址就是报文自己的目的地址，对应配置的cf->via.in struct in_addr src; // cf->src.in， 源地址策略路由匹配 struct netif_port *port; // 出接口 rte_atomic32_t refcnt; }; 路由类型 /* dpvs defined. */ #define RTF_FORWARD 0x0400 #define RTF_LOCALIN 0x0800 #define RTF_DEFAULT 0x1000 #define RTF_KNI 0X2000 #define RTF_OUTWALL 0x4000 路由表类型 #define this_route_lcore (RTE_PER_LCORE(route_lcore)) #define this_local_route_table (this_route_lcore.local_route_table) #define this_net_route_table (this_route_lcore.</p><footer><a href=https://scottlx.github.io/posts/dpvs-route%E8%BD%AC%E5%8F%91/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>Feb 18, 2025
- 1 minute read
- <a href=https://scottlx.github.io/posts/dpvs-session%E5%90%8C%E6%AD%A5/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/dpvs-session%E5%90%8C%E6%AD%A5/>dpvs session 同步</a></h1></header><p>总体架构 lb session同步采用分布式架构，session创建流程触发session数据发送，依次向集群内所有其他节点发送 其他节点收到新的session数据修改本地session表。 session接收和发送各占一个独立线程。
step1: 向所有其他节点发送session数据 remote session：从别的节点同步来的session
local session：本节点收到数据包自己生成的session
step2: session同步至worker 方案一（有锁）： • 独立进程和core处理session同步（per numa） • 每个lcore分配local session和remote session，正常情况下都能直接从local session走掉 • 同步过来的session写到remote session表 • session ip根据fdir走到指定进程
方案二（无锁）： • 独立进程和core处理session同步消息 • 每个lcore 有来local session和remote session，通过owner属性区分。 • 同步过来的session由session_sync core发消息给对应的slave，由对应的slave进行读写，因此可以做到无锁。 • session ip根据fdir走到指定core
session同步具体实现 亟待解决的问题： 同步过来的session什么时候老化？
别的节点上线，本节点要发送哪些session？
别的节点下线，本节点要删除哪些session？
是否要响应下线节点的删除/老化请求？
下线节点怎么知道自己已经下线（数据面）？
解决方案： 方案一： session增加owner属性c owner属性：
conn.owner // indicates who has this session session 同步状态转移图
一条session在一个集群中，应当只有一台机器在使用，所以有一个owner属性，代表这条session被谁拥有，其它所有机器只对这条session的owner发起的增删改查请求做响应。
同步动作 session同步应当时实时的。在以下场景被触发： 新建session 发送方： session新建完成之后：对于tcp，是握手完毕的；对于udp，是第一条连接。 接收方： 接收来自发送方的session，在对应core上新建这条连接，开启老化，老化时间设定为默认时间（1小时）。 fin/rst 发送方： 发送删除session消息 接收方： 接收方接收session，做完校验后在对应core上删除session 老化 发送方： 老化时间超时之后，本地session删除，同时发布老化信息，告知其它lb， 接收方： 其它lb 做完校验后，开始老化这条session。 设备下线 下线后通过控制器更新其他lb的session同步地址信息，不再向该设备同步，同时开始老化全部属于该设备的session。 设备上线（包含设备扩容） 新设备： 新上线设备引流前要接收其他设备的存量session信息，这个功能通过控制器触发完成，控制器感知到新lb上线后通知集群内其他lb向它同步存量session，session数量达到一致时（阿里gw用70%阈值）允许新lb引流。 旧设备： 向目的方发送全部的属于自己的session。</p><footer><a href=https://scottlx.github.io/posts/dpvs-session%E5%90%8C%E6%AD%A5/ rel=full-article>继续阅读 →</a></footer></article><article><header><p class=meta>Feb 18, 2025
- 3 minute read
- <a href=https://scottlx.github.io/posts/dpvs%E6%95%B0%E6%8D%AE%E6%B5%81%E5%88%86%E6%9E%90/#disqus_thread>Comments</a>
- <a class=label href=https://scottlx.github.io/categories/%e6%8a%80%e6%9c%af%e4%bb%8b%e7%bb%8d/>技术介绍</a></p><h1 class=entry-title><a href=https://scottlx.github.io/posts/dpvs%E6%95%B0%E6%8D%AE%E6%B5%81%E5%88%86%E6%9E%90/>dpvs 数据流分析</a></h1></header><p>dpvs ingress流程分析 从 lcore_job_recv_fwd 开始，这个是dpvs收取报文的开始
设备层 dev->flag & NETIF_PORT_FLAG_FORWARD2KNI &mdash;> 则拷贝一份mbuf到kni队列中，这个由命令行和配置文件决定（做流量镜像，用于抓包）
eth层 netif_rcv_mbuf 这里面涉及到vlan的部分不做过多解析
不支持的协议
目前dpvs支持的协议为ipv4, ipv6, arp。 其它报文类型直接丢给内核。其他类型可以看 eth_types。 to_kni
RTE_ARP_OP_REPLY
复制 nworks-1 份mbuf，发送到其它worker的arp_ring上 ( to_other_worker ), 这份报文fwd到 arp协议.
RTE_ARP_OP_REQUEST
这份报文fwd到 arp协议.
arp协议 arp协议处理 neigh_resolve_input
RTE_ARP_OP_REPLY
建立邻居表，记录信息，并且把这个报文送给内核。 to_kni
RTE_ARP_OP_REQUEST
无条件返回网卡的ip以及mac地址 (free arp), netif_xmit 发送到 core_tx_queue
其它op_code
drop
ip层 ipv4协议 (ipv6数据流程上一致) ipv4_rcv
ETH_PKT_OTHERHOST
报文的dmac不是自己， drop
ipv4 协议校验
不通过， drop
下一层协议为 IPPROTO_OSPF
to_kni
INET_HOOK_PRE_ROUTING hook
hook_list: dp_vs_in , dp_vs_prerouting
这两个都与synproxy有关系，但是我们不会启用这个代理，不过需要注意的是syncproxy不通过时会丢包 drop</p><footer><a href=https://scottlx.github.io/posts/dpvs%E6%95%B0%E6%8D%AE%E6%B5%81%E5%88%86%E6%9E%90/ rel=full-article>继续阅读 →</a></footer></article><div class=pagination><a href=/ aria-label=First class=label-pagination><i class="fa fa-angle-double-left fa-lg"></i></a>
<a href=/ class=label-pagination>1</a>
<a href=/page/2/ class=label-pagination>2</a>
<a href=/page/3/ class=label-pagination>3</a>
<a href=/page/2/ aria-label=Next class=label-pagination><i class="fa fa-angle-right fa-lg"></i></a>
<a href=/page/3/ aria-label=Last><i class="fa fa-angle-double-right fa-lg"></i></a></div></div><aside class="sidebar thirds"><section class="first odd"><h1>Who am I</h1><p><p>云原生网络开发 &ndash;> 高级网管(笑</p><p>golang、云计算、SDN、NFV、软件架构</p><p>记录一些工作上的笔记，主要是以太网数据面开发（包括不限于dpdk，ebpf，ovs，dpvs，vpp&mldr;)以及k8s</p><p>也会记录一些web3方面的学习探索</p></p></section><ul class=sidebar-nav><li class=sidebar-nav-item><a target=_blank rel="noopener noreferrer" href=https://github.com/scottlx title=https://github.com/scottlx><i class="fa fa-github fa-3x"></i></a>
<a target=_blank rel="noopener noreferrer" href="https://www.facebook.com/profile.php?id=100009824623685" title="https://www.facebook.com/profile.php?id=100009824623685"><i class="fa fa-facebook fa-3x"></i></a></li></ul><section class=odd></section><section class=even><h1>Recent Posts</h1><ul id=recent_posts></ul></section></aside></div></div><footer role=contentinfo><p>Copyright &copy; 2025 windseek - <a href=https://scottlx.github.io/license/>License</a> -
<span class=credit>Powered by <a target=_blank href=https://gohugo.io rel="noopener noreferrer">Hugo</a> and <a target=_blank href=https://github.com/parsiya/hugo-octopress/ rel="noopener noreferrer">Hugo-Octopress</a> theme.</p></footer></body></html>