---
title: "æ·±å…¥å‰–æcilium monitoræœºåˆ¶"
date: 2025-05-26T17:53:00+08:00
draft: false
tags: ["ebpf", "cilium", "k8s"]
tags_weight: 66
series: ["ebpfç³»åˆ—"]
series_weight: 96
categories: ["æŠ€æœ¯ä»‹ç»"]
categoryes_weight: 96
---

<!-- more -->

### å¯è°ƒè¯•æ€§

æŠ¥æ–‡è½¬å‘é¢ç»„ä»¶ä¸­ï¼Œå¯è°ƒè¯•æ€§ååˆ†å…³é”®ã€‚å¼€å‘é˜¶æ®µå¯èƒ½å¯ä»¥ä½¿ç”¨ gdbï¼ˆebpf ç”šè‡³ä¸èƒ½ç”¨ gdbï¼Œåªèƒ½ç”¨ trace_printkï¼‰ï¼Œlog ç­‰æ–¹å¼è¿›è¡Œè°ƒè¯•ï¼Œä½†åˆ°äº†ç”Ÿäº§ç¯å¢ƒï¼Œä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½æ˜¯å¿…é¡»è¦å®Œå¤‡çš„ï¼š

- æŠ“åŒ…æ‰‹æ®µ

  - æŒ‰ç…§ç½‘å¡æŠ“åŒ…
  - æŒ‰ç…§æµè¿›è¡ŒæŠ“åŒ…
  - æŒ‰ç…§ç‰¹å®šè¿‡æ»¤æ¡ä»¶æŠ“åŒ…ï¼Œä¾‹å¦‚æºç›®çš„åœ°å€ï¼Œç«¯å£ï¼Œåè®®å·ç­‰

- æŠ¥æ–‡è®¡æ•°
  - æ”¶å‘åŒ…è®¡æ•°ï¼šrxï¼Œtx é˜¶æ®µè®¡æ•°
  - ä¸¢åŒ…è®¡æ•°ï¼šæŒ‰ç…§é”™è¯¯ç è¿›è¡ŒåŒºåˆ†
  - ç‰¹å®šè§‚æµ‹ç‚¹è®¡æ•°ï¼šä¸€äº›é‡è¦è½¬å‘å‡½æ•°ï¼Œä¾‹å¦‚ l3_fwd, arp_response ç­‰
- æµæ—¥å¿—
  - æµé‡æ–¹å‘ï¼šegress/ingress
  - session ä¿¡æ¯ï¼šäº”å…ƒç»„ï¼Œnat ä¿¡æ¯ï¼Œtcp çŠ¶æ€ç­‰
  - å…¶ä»–å¿…è¦çš„ä¸Šä¸‹æ–‡ï¼šä¾‹å¦‚è½¬å‘è¡¨é¡¹æŸ¥æ‰¾çš„ç»“æœï¼Œæ„é€ çš„ actionï¼Œç¡¬ä»¶å¸è½½æ ‡è®°ç­‰

### linux perf_events

![img](/img/blobs/linuxperfevent.png)

ebpf perf åŸºäº linux perf_event å­ç³»ç»Ÿã€‚epbf é€šçŸ¥ç”¨æˆ·æ€æ‹·è´æ•°æ®æ—¶åŸºäº perf_events çš„

### perf buffer

ebpf ä¸­æä¾›äº†å†…æ ¸å’Œç”¨æˆ·ç©ºé—´ä¹‹é—´é«˜æ•ˆåœ°äº¤æ¢æ•°æ®çš„æœºåˆ¶ï¼šperf bufferã€‚å®ƒæ˜¯ä¸€ç§ per-cpu çš„ç¯å½¢ç¼“å†²åŒºï¼Œå½“æˆ‘ä»¬éœ€è¦å°† ebpf æ”¶é›†åˆ°çš„æ•°æ®å‘é€åˆ°ç”¨æˆ·ç©ºé—´è®°å½•æˆ–è€…å¤„ç†æ—¶ï¼Œå°±å¯ä»¥ç”¨ perf buffer æ¥å®Œæˆã€‚å®ƒè¿˜æœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼š

1. èƒ½å¤Ÿè®°å½•å¯å˜é•¿åº¦æ•°æ®è®°ï¼›
2. èƒ½å¤Ÿé€šè¿‡å†…å­˜æ˜ å°„çš„æ–¹å¼åœ¨ç”¨æˆ·æ€è¯»å–è¯»å–æ•°æ®ï¼Œè€Œæ— éœ€é€šè¿‡ç³»ç»Ÿè°ƒç”¨é™·å…¥åˆ°å†…æ ¸å»æ‹·è´æ•°æ®ï¼›
3. å®ç° epoll é€šçŸ¥æœºåˆ¶

å› æ­¤åœ¨ cilium ä¸­ï¼Œå®ç°ä¸Šè¿°è°ƒè¯•æ‰‹æ®µçš„æ€è·¯ï¼Œå°±æ˜¯åœ¨è½¬å‘é¢ä»£ç ä¸­æ„é€ ç›¸åº”çš„ event åˆ°`EVENTS_MAP`ï¼Œä¹‹åé€šè¿‡åˆ«çš„å·¥å…·å»è¯»å–å¹¶è§£æ`EVENTS_MAP`ä¸­çš„æ•°æ®

EVENTS_MAP å®šä¹‰å¦‚ä¸‹: bpf/lib/events.h

```c
struct {
	__uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY);
	__uint(key_size, sizeof(__u32));
	__uint(value_size, sizeof(__u32));
	__uint(pinning, LIBBPF_PIN_BY_NAME);
	__uint(max_entries, __NR_CPUS__);
} EVENTS_MAP __section_maps_btf;

```

key æ˜¯ cpu çš„ç¼–å·ï¼Œå› æ­¤å¤§å°æ˜¯ u32ï¼›value ä¸€èˆ¬æ˜¯æ–‡ä»¶æè¿°ç¬¦ fdï¼Œå…³è”ä¸€ä¸ª perf eventï¼Œå› æ­¤ä¹Ÿæ˜¯ u32

æ•°æ®é¢ä»£ç æ„é€ å¥½ data ä¹‹åï¼Œä½¿ç”¨ helper function: `bpf_perf_event_output`é€šçŸ¥ç”¨æˆ·æ€ä»£ç æ‹·è´æ•°æ®

ä¸‹é¢æ˜¯ cilium ä»£ç ä¸­å°è£…å¥½çš„ event è¾“å‡ºå‡½æ•°ï¼Œæœ€ç»ˆå°±æ˜¯è°ƒç”¨çš„ bpf_perf_event_output

```c
// bpf/include/bpf/ctx/skb.h
#define ctx_event_output	skb_event_output

// bpf/include/bpf/helpers_skb.h
/* Events for user space */
static int BPF_FUNC_REMAP(skb_event_output, struct __sk_buff *skb, void *map,
			  __u64 index, const void *data, __u32 size) =
			 (void *)BPF_FUNC_perf_event_output; //å¯¹åº”çš„func id æ˜¯ 25

// /usr/include/linux/bpf.h
/* integer value in 'imm' field of BPF_CALL instruction selects which helper
 * function eBPF program intends to call
 */
#define __BPF_ENUM_FN(x) BPF_FUNC_ ## x
enum bpf_func_id {
	__BPF_FUNC_MAPPER(__BPF_ENUM_FN)
	__BPF_FUNC_MAX_ID,
};
#undef __BPF_ENUM_FN
```

### è½¬å‘é¢ç”Ÿæˆ perf

debugï¼Œdrop notifyï¼Œtrace éƒ½åªæ˜¯ä¸åŒçš„æ•°æ®æ ¼å¼ï¼Œæœ€ç»ˆéƒ½æ˜¯è°ƒç”¨`ctx_event_output`ç”Ÿæˆ event

æ•°æ®æ ¼å¼ä¾é  common header çš„ type è¿›è¡ŒåŒºåˆ†

```c
// bpf/lib/common.h
#define NOTIFY_COMMON_HDR \
	__u8		type;		\
	__u8		subtype;	\
	__u16		source;		\
	__u32		hash;

//type å®šä¹‰
enum {
	CILIUM_NOTIFY_UNSPEC,
	CILIUM_NOTIFY_DROP,
	CILIUM_NOTIFY_DBG_MSG,
	CILIUM_NOTIFY_DBG_CAPTURE,
	CILIUM_NOTIFY_TRACE,
	CILIUM_NOTIFY_POLICY_VERDICT,
	CILIUM_NOTIFY_CAPTURE,
	CILIUM_NOTIFY_TRACE_SOCK,
};
```

subtypeï¼Œsourceï¼Œhash è¿™ä¸‰ä¸ªå­—æ®µï¼Œä¸åŒçš„ type æœ‰å„è‡ªä¸åŒçš„ç”¨æ³•ï¼Œåé¢ä¼šæåˆ°

#### Debug æ—¥å¿—

debug åˆ†ä¸¤ç§ï¼Œ

- ç®€å•çš„ä¼ å‚ï¼Œåªä¼ é€’ 2 ä¸ªæˆ– 3 ä¸ª u32 åˆ°ç”¨æˆ·æ€
- å¸¦ capture çš„ï¼Œå°†æ•´ä¸ª\_\_ctx_buff æŠ¥æ–‡å¸¦åˆ°ç”¨æˆ·æ€ç©ºé—´

```c
// bpf/lib/dbg.h
// åªå¸¦argçš„ï¼Œcommon headeråç›´æ¥åŠ data
static __always_inline void cilium_dbg(struct __ctx_buff *ctx, __u8 type,
				       __u32 arg1, __u32 arg2)
{
	struct debug_msg msg = {
		__notify_common_hdr(CILIUM_NOTIFY_DBG_MSG, type),
		.arg1	= arg1,
		.arg2	= arg2,
	};

	ctx_event_output(ctx, &EVENTS_MAP, BPF_F_CURRENT_CPU,
			 &msg, sizeof(msg));
}

// å¸¦captureçš„ï¼Œcommon headeråå¸¦äº†pktcap_hdrï¼ŒæŒ‡å®šåŸåŒ…é•¿å’ŒæŠ“åŒ…çš„åŒ…é•¿
static __always_inline void cilium_dbg_capture2(struct __ctx_buff *ctx, __u8 type,
						__u32 arg1, __u32 arg2)
{
	__u64 ctx_len = ctx_full_len(ctx);
	__u64 cap_len = min_t(__u64, TRACE_PAYLOAD_LEN, ctx_len);
	struct debug_capture_msg msg = {
		__notify_common_hdr(CILIUM_NOTIFY_DBG_CAPTURE, type),
		__notify_pktcap_hdr((__u32)ctx_len, (__u16)cap_len, NOTIFY_CAPTURE_VER),
		.arg1	= arg1,
		.arg2	= arg2,
	};

	ctx_event_output(ctx, &EVENTS_MAP,
			 (cap_len << 32) | BPF_F_CURRENT_CPU,
			 &msg, sizeof(msg));
}

```

å…¶ä¸­ typeï¼ˆcommon_header ä¸­çš„ subtypeï¼‰å®šä¹‰äº†ç”¨æˆ·æ€ä»£ç åœ¨è§£ææ—¶çš„è¾“å‡ºæ ¼å¼ï¼Œç”± monitor è¿›è¡Œæ ¼å¼åŒ–è¾“å‡º

```go
// pkg/monitor/datapath_debug.go
// Message returns the debug message in a human-readable format
func (n *DebugMsg) Message(linkMonitor getters.LinkGetter) string {
	switch n.SubType {
	case DbgGeneric:
		return fmt.Sprintf("No message, arg1=%d (%#x) arg2=%d (%#x)", n.Arg1, n.Arg1, n.Arg2, n.Arg2)
	case DbgLocalDelivery:
		return fmt.Sprintf("Attempting local delivery for container id %d from seclabel %d", n.Arg1, n.Arg2)
	case DbgEncap:
		return fmt.Sprintf("Encapsulating to node %d (%#x) from seclabel %d", n.Arg1, n.Arg1, n.Arg2)
	case DbgLxcFound:
		var ifname string
		if linkMonitor != nil {
			ifname = linkMonitor.Name(n.Arg1)
		}
		return fmt.Sprintf("Local container found ifindex %s seclabel %d", ifname, byteorder.NetworkToHost16(uint16(n.Arg2)))
	case DbgPolicyDenied:
		return fmt.Sprintf("Policy evaluation would deny packet from %d to %d", n.Arg1, n.Arg2)
	case DbgCtLookup:
		return fmt.Sprintf("CT lookup: %s", ctInfo(n.Arg1, n.Arg2))
	case DbgCtLookupRev:
		return fmt.Sprintf("CT reverse lookup: %s", ctInfo(n.Arg1, n.Arg2))
	case DbgCtLookup4:
        // ...
```

#### Drop notification

drop notification æ˜¯ä¸€ç§å¸¦äº†æ›´å¤šä¿¡æ¯çš„ debug captureï¼Œæ•°æ®æ ¼å¼å¦‚ä¸‹

```c
// bpf/lib/drop.h
struct drop_notify {
	NOTIFY_CAPTURE_HDR
	__u32		src_label; /* identifaction labels */
	__u32		dst_label;
	__u32		dst_id; /* 0 for egress */
	__u16		line;  /* å‘ç”Ÿä¸¢åŒ…çš„ä»£ç è¡Œ */
	__u8		file;  /* å‘ç”Ÿä¸¢åŒ…çš„æ–‡ä»¶å */
	__s8		ext_error; /* æ‰©å±•é”™è¯¯ç  */
	__u32		ifindex;  /* ä¸¢åŒ…ç½‘å¡ */
};
```

æ–‡ä»¶åå’Œä»£ç è¡Œæ˜¯ç¼–è¯‘å™¨å†…ç½®å®è¾“å‡ºçš„

```c
// bpf/lib/source_info.h

#define __MAGIC_FILE__ (__u8)__id_for_file(__FILE_NAME__)
#define __MAGIC_LINE__ __LINE__

#define _strcase_(id, known_name) do {			\
	if (!__builtin_strcmp(header_name, known_name))	\
		return id;				\
	} while (0)

/*
 * __id_for_file is used by __MAGIC_FILE__ to encode source file information in
 * drop notifications and forward/drop metrics. It must be inlined, otherwise
 * clang won't translate this to a constexpr.
 *
 * The following list of files is static, but it is validated during build with
 * the pkg/datapath/loader/check-sources.sh tool.
 */
static __always_inline int
__id_for_file(const char *const header_name)
{
	/* @@ source files list begin */

	/* source files from bpf/ */
	_strcase_(1, "bpf_host.c");
	_strcase_(2, "bpf_lxc.c");
	_strcase_(3, "bpf_overlay.c");
	_strcase_(4, "bpf_xdp.c");
	_strcase_(5, "bpf_sock.c");
	_strcase_(6, "bpf_network.c");
	_strcase_(7, "bpf_wireguard.c");

	/* header files from bpf/lib/ */
	_strcase_(101, "arp.h");
	_strcase_(102, "drop.h");
	_strcase_(103, "srv6.h");
	_strcase_(104, "icmp6.h");
	_strcase_(105, "nodeport.h");
	_strcase_(106, "lb.h");
	_strcase_(107, "mcast.h");
	_strcase_(108, "ipv4.h");
	_strcase_(109, "conntrack.h");
	_strcase_(110, "l3.h");
	_strcase_(111, "trace.h");
	_strcase_(112, "encap.h");
	_strcase_(113, "encrypt.h");
	_strcase_(114, "host_firewall.h");
	_strcase_(115, "nodeport_egress.h");

	/* @@ source files list end */

	return 0;
}
```

ç”¨æˆ·æ€è§£ææ—¶ï¼Œæ–‡ä»¶ç¼–å·éœ€è¦å¯¹åº”ä¸Šï¼Œå¯ä»¥é€šè¿‡`contrib/scripts/check-source-info.sh`è¿™ä¸ªè„šæœ¬æ¥ç¡®ä¿ä¸¤ä¸ªæ–‡ä»¶æ˜¯å¯¹åº”ä¸Šçš„

```go
// pkg/monitor/api/files.go

// Keep in sync with __id_for_file in bpf/lib/source_info.h.
var files = map[uint8]string{
	// @@ source files list begin

	// source files from bpf/
	1: "bpf_host.c",
	2: "bpf_lxc.c",
	3: "bpf_overlay.c",
	4: "bpf_xdp.c",
	5: "bpf_sock.c",
	6: "bpf_network.c",
	7: "bpf_wireguard.c",

	// header files from bpf/lib/
	101: "arp.h",
	102: "drop.h",
	103: "srv6.h",
	104: "icmp6.h",
	105: "nodeport.h",
	106: "lb.h",
	107: "mcast.h",
	108: "ipv4.h",
	109: "conntrack.h",
	110: "l3.h",
	111: "trace.h",
	112: "encap.h",
	113: "encrypt.h",
	114: "host_firewall.h",
	115: "nodeport_egress.h",

	// @@ source files list end
}

// BPFFileName returns the file name for the given BPF file id.
func BPFFileName(id uint8) string {
	if name, ok := files[id]; ok {
		return name
	}
	return fmt.Sprintf("unknown(%d)", id)
}

```

ç›¸åŒçš„ï¼Œ`bpf/lib/common.h`å’Œ`pkg/monitor/api/drop.go`çš„é”™è¯¯ç ä¹Ÿè¦å¯¹åº”ä¸Š

#### trace

æ•°æ®æ ¼å¼å¦‚ä¸‹

```c
struct trace_notify {
	NOTIFY_CAPTURE_HDR
	__u32		src_label;
	__u32		dst_label;
	__u16		dst_id;
	__u8		reason;
	__u8		ipv6:1;
	__u8		pad:7;
	__u32		ifindex;
	union {
		struct {
			__be32		orig_ip4;
			__u32		orig_pad1;
			__u32		orig_pad2;
			__u32		orig_pad3;
		};
		union v6addr	orig_ip6;
	};
};
```

è½¬å‘ reason æœ‰ä»¥ä¸‹å‡ ç§ï¼Œä¸ conntrack çŠ¶æ€å¼ºç›¸å…³

```c
// bpf/lib/trace.h
/* Reasons for forwarding a packet, keep in sync with pkg/monitor/datapath_trace.go */
enum trace_reason {
	TRACE_REASON_POLICY = CT_NEW,
	TRACE_REASON_CT_ESTABLISHED = CT_ESTABLISHED,
	TRACE_REASON_CT_REPLY = CT_REPLY,
	TRACE_REASON_CT_RELATED = CT_RELATED,
	TRACE_REASON_RESERVED,
	TRACE_REASON_UNKNOWN,
	TRACE_REASON_SRV6_ENCAP,
	TRACE_REASON_SRV6_DECAP,
	TRACE_REASON_ENCRYPT_OVERLAY,
	/* Note: TRACE_REASON_ENCRYPTED is used as a mask. Beware if you add
	 * new values below it, they would match with that mask.
	 */
	TRACE_REASON_ENCRYPTED = 0x80,
} __packed;
```

è§‚æµ‹ç‚¹

```c
// bpf/lib/trace.h
enum trace_point {
	TRACE_TO_LXC,
	TRACE_TO_PROXY,
	TRACE_TO_HOST,
	TRACE_TO_STACK,
	TRACE_TO_OVERLAY,
	TRACE_FROM_LXC,
	TRACE_FROM_PROXY,
	TRACE_FROM_HOST,
	TRACE_FROM_STACK,
	TRACE_FROM_OVERLAY,
	TRACE_FROM_NETWORK,
	TRACE_TO_NETWORK,
} __packed;
```

trace çš„è§‚æµ‹ç‚¹æ˜¯ä¿å­˜åœ¨ common header çš„ subtype å­—æ®µï¼Œä»£ç å¦‚ä¸‹

```c
#define send_trace_notify(ctx, obs_point, src, dst, dst_id, ifindex, reason, monitor) \
		_send_trace_notify(ctx, obs_point, src, dst, dst_id, ifindex, reason, monitor, \
		__MAGIC_LINE__, __MAGIC_FILE__)
static __always_inline void
_send_trace_notify(struct __ctx_buff *ctx, enum trace_point obs_point,
		   __u32 src, __u32 dst, __u16 dst_id, __u32 ifindex,
		   enum trace_reason reason, __u32 monitor, __u16 line, __u8 file)
{
	__u64 ctx_len = ctx_full_len(ctx);
	__u64 cap_len = min_t(__u64, monitor ? : TRACE_PAYLOAD_LEN,
			      ctx_len);
	struct ratelimit_key rkey = {
		.usage = RATELIMIT_USAGE_EVENTS_MAP,
	};
	struct ratelimit_settings settings = {
		.topup_interval_ns = NSEC_PER_SEC,
	};
	struct trace_notify msg __align_stack_8;

	_update_trace_metrics(ctx, obs_point, reason, line, file); // æ›´æ–°metricsè®¡æ•°

	if (!emit_trace_notify(obs_point, monitor))
		return;

	if (EVENTS_MAP_RATE_LIMIT > 0) {  // é˜²æ­¢æŠ¥æ–‡è¿‡å¤šæ‰“çˆ†ringç¯
		settings.bucket_size = EVENTS_MAP_BURST_LIMIT;
		settings.tokens_per_topup = EVENTS_MAP_RATE_LIMIT;
		if (!ratelimit_check_and_take(&rkey, &settings))
			return;
	}

	msg = (typeof(msg)) {
		__notify_common_hdr(CILIUM_NOTIFY_TRACE, obs_point), // subtypeæ˜¯obs_point
		__notify_pktcap_hdr((__u32)ctx_len, (__u16)cap_len, NOTIFY_CAPTURE_VER),
		.src_label	= src,
		.dst_label	= dst,
		.dst_id		= dst_id,
		.reason		= reason,
		.ifindex	= ifindex,
	};
	memset(&msg.orig_ip6, 0, sizeof(union v6addr));

	ctx_event_output(ctx, &EVENTS_MAP,
			 (cap_len << 32) | BPF_F_CURRENT_CPU,
			 &msg, sizeof(msg));
}
```

### monitor socket

cilium daemon å¯åŠ¨æ—¶ï¼Œä¼šå¯åŠ¨ monitor-agentï¼Œè¯»å– perf event ring å¹¶æä¾› api ç»™ cilium-dbg å·¥å…·æˆ– envoy è¿›è¡Œè¿æ¥

![cilium event](/img/blobs/ciliumevent.png)

#### agent ç»„ä»¶

```go
// pkg/monitor/agent/agent.go
type agent struct {
	lock.Mutex
	models.MonitorStatus

	ctx              context.Context
	perfReaderCancel context.CancelFunc

	// listeners are external cilium monitor clients which receive raw
	// gob-encoded payloads
	listeners map[listener.MonitorListener]struct{}
	// consumers are internal clients which receive decoded messages
	consumers map[consumer.MonitorConsumer]struct{}

	events        *ebpf.Map
	monitorEvents *perf.Reader
}
```

#### è¯»å– perf ring æµç¨‹

```go
func (a *agent) handleEvents(stopCtx context.Context) {
	scopedLog := log.WithField(logfields.StartTime, time.Now())
	scopedLog.Info("Beginning to read perf buffer")
	defer scopedLog.Info("Stopped reading perf buffer")

	bufferSize := int(a.Pagesize * a.Npages)
	monitorEvents, err := perf.NewReader(a.events, bufferSize) //åˆå§‹åŒ–reader
	if err != nil {
		scopedLog.WithError(err).Fatal("Cannot initialise BPF perf ring buffer sockets")
	}
	defer func() {
		monitorEvents.Close()
		a.Lock()
		a.monitorEvents = nil
		a.Unlock()
	}()

	a.Lock()
	a.monitorEvents = monitorEvents
	a.Unlock()

	for !isCtxDone(stopCtx) {
		record, err := monitorEvents.Read()
		switch {
		case isCtxDone(stopCtx):
			return
		case err != nil:
			if perf.IsUnknownEvent(err) {
				a.Lock()
				a.MonitorStatus.Unknown++
				a.Unlock()
			} else {
				scopedLog.WithError(err).Warn("Error received while reading from perf buffer")
				if errors.Is(err, unix.EBADFD) {
					return
				}
			}
			continue
		}

		a.processPerfRecord(scopedLog, record) // è§£ææ¯ä¸ªevent
	}
}
// processPerfRecord processes a record from the datapath and sends it to any
// registered subscribers
func (a *agent) processPerfRecord(scopedLog *logrus.Entry, record perf.Record) {
	a.Lock()
	defer a.Unlock()

	if record.LostSamples > 0 {
		a.MonitorStatus.Lost += int64(record.LostSamples)
		a.notifyPerfEventLostLocked(record.LostSamples, record.CPU)
		a.sendToListenersLocked(&payload.Payload{
			CPU:  record.CPU,
			Lost: record.LostSamples,
			Type: payload.RecordLost,
		})

	} else {
		a.notifyPerfEventLocked(record.RawSample, record.CPU)
		a.sendToListenersLocked(&payload.Payload{ // å¹¿æ’­åˆ°æ‰€æœ‰listenerï¼Œä¹Ÿå°±æ˜¯å®¢æˆ·ç«¯
			Data: record.RawSample,
			CPU:  record.CPU, // æ¯ä¸ªcpuéƒ½æœ‰ä¸€ä¸ªrecordï¼Œæ˜¯ç‹¬ç«‹çš„
			Type: payload.EventSample,
		})
	}
}

```

æ¯ä¸ªè¿æ¥çš„ client éƒ½ä¼šåˆ›å»ºä¸€ä¸ª listenerã€‚listener ä¼šåˆ†é…ä¸€ä¸ªé˜Ÿåˆ—ã€‚å½“ä¸€ä¸ª event ç”Ÿæˆåï¼Œevent ä¼šè¢«å¹¿æ’­åˆ°æ‰€æœ‰ listener çš„é˜Ÿåˆ—ä¸­ï¼Œé˜Ÿåˆ—ä¸­çš„ event ä¼šè¢« listener æ¶ˆè´¹å¹¶å‘é€ç»™ clientã€‚å‘é€ç»™ client çš„æ•°æ®éƒ½æ˜¯ raw dataï¼Œéœ€è¦ client è‡ªè¡Œè§£æ

```go
// listenerv1_2 implements the cilium-node-monitor API protocol compatible with
// cilium 1.2
// cleanupFn is called on exit
type listenerv1_2 struct {
	conn      net.Conn
	queue     chan *payload.Payload
	cleanupFn func(listener.MonitorListener)
	// Used to prevent queue from getting closed multiple times.
	once sync.Once
}

func newListenerv1_2(c net.Conn, queueSize int, cleanupFn func(listener.MonitorListener)) *listenerv1_2 {
	ml := &listenerv1_2{
		conn:      c,
		queue:     make(chan *payload.Payload, queueSize),
		cleanupFn: cleanupFn,
	}

	go ml.drainQueue()

	return ml
}

func (ml *listenerv1_2) drainQueue() {
	defer func() {
		ml.cleanupFn(ml)
	}()

	enc := gob.NewEncoder(ml.conn)
	for pl := range ml.queue {
		if err := pl.EncodeBinary(enc); err != nil { //å†™åˆ°socket
			switch {
			case listener.IsDisconnected(err):
				log.Debug("Listener disconnected")
				return

			default:
				log.WithError(err).Warn("Removing listener due to write failure")
				return
			}
		}
	}
}
```

å…·ä½“æŠ¥æ–‡è§£æä»£ç ä½äº`pkg/monitor/format/format.go`

#### perf.Reader çš„å®ç°

ä¸‹é¢è¯¦ç»†å±•å¼€ä¸€ä¸‹ cilium å¯¹äº perf.Reader çš„å®ç°

ä¸»è¦æµç¨‹ï¼š

1. ä¸ºæ¯ä¸ª cpu åˆ›å»º perf event
2. perf event çš„ fd åš mmap æ˜ å°„ï¼Œæ‹¿åˆ°å†…å­˜åœ°å€
3. fd åŠ åˆ° epoll
4. å¯åŠ¨ readIntoï¼Œå¤§éƒ¨åˆ†æ—¶å€™ epoll wait ç­‰å¾…ï¼Œç›´åˆ°æœ‰ epoll äº‹ä»¶æ—¶ä» ring ä¸­è¯»å– event

```go
func NewReaderWithOptions(array *ebpf.Map, perCPUBuffer int, opts ReaderOptions) (pr *Reader, err error) {
	closeOnError := func(c io.Closer) {
		if err != nil {
			c.Close()
		}
	}

	if perCPUBuffer < 1 {
		return nil, errors.New("perCPUBuffer must be larger than 0")
	}
	if opts.WakeupEvents > 0 && opts.Watermark > 0 {
		return nil, errors.New("WakeupEvents and Watermark cannot both be non-zero")
	}

	var (
		nCPU     = int(array.MaxEntries())
		rings    = make([]*perfEventRing, 0, nCPU)
		eventFds = make([]*sys.FD, 0, nCPU)
	)

	poller, err := epoll.New() // ä½¿ç”¨epollè¯»å–fd
	if err != nil {
		return nil, err
	}
	defer closeOnError(poller)

	// bpf_perf_event_output checks which CPU an event is enabled on,
	// but doesn't allow using a wildcard like -1 to specify "all CPUs".
	// Hence we have to create a ring for each CPU.
	bufferSize := 0
	for i := 0; i < nCPU; i++ { //ç¼–è¯‘æ‰€æœ‰å¯èƒ½çš„cpu
		event, ring, err := newPerfEventRing(i, perCPUBuffer, opts)
		if errors.Is(err, unix.ENODEV) {
			// The requested CPU is currently offline, skip it.
			continue
		}

		if err != nil {
			return nil, fmt.Errorf("failed to create perf ring for CPU %d: %v", i, err)
		}
		defer closeOnError(event)
		defer closeOnError(ring)

		bufferSize = ring.size()
		rings = append(rings, ring)
		eventFds = append(eventFds, event)
		/* å°†fdåŠ å…¥åˆ°epoll */
		if err := poller.Add(event.Int(), 0); err != nil {
			return nil, err
		}
	}

	// Closing a PERF_EVENT_ARRAY removes all event fds
	// stored in it, so we keep a reference alive.
	array, err = array.Clone()
	if err != nil {
		return nil, err
	}

	pr = &Reader{
		array:        array,
		rings:        rings,
		poller:       poller,
		deadline:     time.Time{},
		epollEvents:  make([]unix.EpollEvent, len(rings)),
		epollRings:   make([]*perfEventRing, 0, len(rings)),
		eventHeader:  make([]byte, perfEventHeaderSize),
		eventFds:     eventFds,
		overwritable: opts.Overwritable,
		bufferSize:   bufferSize,
	}
	if err = pr.Resume(); err != nil {
		return nil, err
	}
	runtime.SetFinalizer(pr, (*Reader).Close)
	return pr, nil
}

func newPerfEventRing(cpu, perCPUBuffer int, opts ReaderOptions) (_ *sys.FD, _ *perfEventRing, err error) {
	closeOnError := func(c io.Closer) {
		if err != nil {
			c.Close()
		}
	}

	if opts.Watermark >= perCPUBuffer {
		return nil, nil, errors.New("watermark must be smaller than perCPUBuffer")
	}

	fd, err := createPerfEvent(cpu, opts) //åˆ›å»ºperfEventï¼Œå¾—åˆ°å¯¹åº”çš„fd
	if err != nil {
		return nil, nil, err
	}
	defer closeOnError(fd)

	if err := unix.SetNonblock(fd.Int(), true); err != nil {
		return nil, nil, err
	}

	protections := unix.PROT_READ
	if !opts.Overwritable {
		protections |= unix.PROT_WRITE
	}

	mmap, err := unix.Mmap(fd.Int(), 0, perfBufferSize(perCPUBuffer), protections, unix.MAP_SHARED) // mmapåˆ°è¯¥ringçš„åœ°å€ç©ºé—´
	if err != nil {
		return nil, nil, fmt.Errorf("can't mmap: %v", err)
	}

	// This relies on the fact that we allocate an extra metadata page,
	// and that the struct is smaller than an OS page.
	// This use of unsafe.Pointer isn't explicitly sanctioned by the
	// documentation, since a byte is smaller than sampledPerfEvent.
	meta := (*unix.PerfEventMmapPage)(unsafe.Pointer(&mmap[0]))

	var reader ringReader
	if opts.Overwritable {
		reader = newReverseReader(meta, mmap[meta.Data_offset:meta.Data_offset+meta.Data_size])
	} else {
		reader = newForwardReader(meta, mmap[meta.Data_offset:meta.Data_offset+meta.Data_size])
	}

	ring := &perfEventRing{
		cpu:        cpu,
		mmap:       mmap,
		ringReader: reader,
	}
	runtime.SetFinalizer(ring, (*perfEventRing).Close)

	return fd, ring, nil
}



func (pr *Reader) ReadInto(rec *Record) error {
	pr.mu.Lock()
	defer pr.mu.Unlock()

	pr.pauseMu.Lock()
	defer pr.pauseMu.Unlock()

	if pr.overwritable && !pr.paused {
		return errMustBePaused
	}

	if pr.rings == nil {
		return fmt.Errorf("perf ringbuffer: %w", ErrClosed)
	}

	for {
		if len(pr.epollRings) == 0 {
			if pe := pr.pendingErr; pe != nil {
				// All rings have been emptied since the error occurred, return
				// appropriate error.
				pr.pendingErr = nil
				return pe
			}

			// NB: The deferred pauseMu.Unlock will panic if Wait panics, which
			// might obscure the original panic.
			pr.pauseMu.Unlock()
			_, err := pr.poller.Wait(pr.epollEvents, pr.deadline)
			pr.pauseMu.Lock()

			if errors.Is(err, os.ErrDeadlineExceeded) || errors.Is(err, ErrFlushed) {
				// We've hit the deadline, check whether there is any data in
				// the rings that we've not been woken up for.
				pr.pendingErr = err
			} else if err != nil {
				return err
			}

			// Re-validate pr.paused since we dropped pauseMu.
			if pr.overwritable && !pr.paused {
				return errMustBePaused
			}

			// Waking up userspace is expensive, make the most of it by checking
			// all rings.
			for _, ring := range pr.rings {
				ring.loadHead()
				pr.epollRings = append(pr.epollRings, ring)
			}
		}

		// Start at the last available event. The order in which we
		// process them doesn't matter, and starting at the back allows
		// resizing epollRings to keep track of processed rings.
		err := pr.readRecordFromRing(rec, pr.epollRings[len(pr.epollRings)-1])
		if err == errEOR {
			// We've emptied the current ring buffer, process
			// the next one.
			pr.epollRings = pr.epollRings[:len(pr.epollRings)-1]
			continue
		}

		return err
	}
}
```

### monitor æ ·ä¾‹è¾“å‡º

æœ¬äººå¼€å‘çš„åŸºäº cilium çš„é­”æ”¹ç‰ˆæœ¬ ğŸ˜‰ï¼ˆå®ç°åŸºæœ¬çš„ vpc åŠŸèƒ½ï¼‰

![trace](/img/blobs/trace.PNG)
