<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ebpf on windseek</title><link>https://scottlx.github.io/tags/ebpf/</link><description>Recent content in ebpf on windseek</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 30 May 2025 10:15:00 +0800</lastBuildDate><atom:link href="https://scottlx.github.io/tags/ebpf/index.xml" rel="self" type="application/rss+xml"/><item><title>cilium endpoint 创建流程</title><link>https://scottlx.github.io/posts/cilium-endpoint-%E5%88%9B%E5%BB%BA%E6%B5%81%E7%A8%8B/</link><pubDate>Fri, 30 May 2025 10:15:00 +0800</pubDate><guid>https://scottlx.github.io/posts/cilium-endpoint-%E5%88%9B%E5%BB%BA%E6%B5%81%E7%A8%8B/</guid><description>pod创建后，cilium打通网络涉及以下内容：
lxc网卡的创建（cni插件） bpf代码的加载（agent） ipam 地址分配（agent） endpoint CR的创建（agent） endpoint生命周期管理（agent） 具体流程如下图
流程说明
cni add流程主要分三步：
调用ipam接口从agent获取ip信息 创建lxc网卡（veth），根据ip信息配置网卡（mtu，gso，gro配置），容器命名空间内路由等 调用endpointCreate接口通知agent开始接管lxc网卡（加载bpf代码等） 如果启用了cniChaning，还会去执行chaining的动作
ipam流程将在后期详细介绍，本篇主要分析endpointCreate之后的流程，也就是bpf代码是如何加载到lxc网卡上的。
cilium controller cilium agent代码内部，对于资源同步的场景，设计了一套controller框架。
controller可以理解为异步任务控制器，在后台尝试某一对象的同步任务直到成功，并记录成功失败次数，错误日志等监控数据。每个controller对应一个协程。
controller需要被manager绑定，而manager则绑定到某一特定资源，比如endpoint
由于资源的变配会需要多个异步任务的执行，因此一个manager可以关联多个controller，单个controller只负责某一特定的异步任务(只要是可能失败并需要重试的任务都适用，例如给k8s资源打annotation，同步对象到某个存储，kvstore，bpfmap等）
controller之间通信通过eventqueue进行异步解耦。每个evq对应一个协程
endpoint manager的架构图 说明
上图中，endpoint manager维护了本节点endpoint列表，并实现了
gc controller：endpoint定期清理，清理不健康的endpoint。 regenerate controller：endpoint定期全量重建，重建ep对应的policy和configuration CNI创建endpoint后，endpoint对象会被创建。每个endpoint初始化时会有一个eventQueue和处理该eventQueue的一个controller。endpoint manager会将regen事件入队到endpoint的eventQueue中，并启动endpoint的sync Controller。sync Controller会同步ep信息到k8s cep CR，这样用户就可以从apiserver获取endpoint状态了
ebpf程序加载流程 eventQueue中的regeneration event会触发endpoint的重建，也就是相关ebpf程序的编译加载和ebpf map数据的插入。
bpf程序加载会发生在几种情况下：
第一次创建时进行初始化 cilium重启时，会进行一次regenerate（按需初始化） 用户执行cilium endpoint regenerate 时（按需） 用户执行ciluim config时（按需） 程序是否加载由编译级别来控制
0 -&amp;gt; “invalid” (未设置)
1：RegenerateWithoutDatapath -&amp;gt; “no-rebuild” （更新policy，dns，只需更新map，不重新编译加载bpf）
2：RegenerateWithDatapath -&amp;gt; &amp;ldquo;rewrite+load&amp;rdquo; （新创建endpoint）
这边可能会有个疑问，为什么更新policy不是更新lxc代码。这是因为policy代码是在bpf_lxc的最后使用尾调用执行的。因此更新policy只要更新prog bpf，而不需要动已经加载在lxc上的代码
static __always_inline int l3_local_delivery(struct __ctx_buff *ctx, __u32 seclabel, __u32 magic __maybe_unused, const struct endpoint_info *ep __maybe_unused, __u8 direction __maybe_unused, bool from_host __maybe_unused, bool from_tunnel __maybe_unused, __u32 cluster_id __maybe_unused) { /*省略一些代码*/ /* Jumps to destination pod&amp;#39;s BPF program to enforce ingress policies.</description></item><item><title>深入剖析cilium monitor机制</title><link>https://scottlx.github.io/posts/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90cilium-monitor%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 26 May 2025 17:53:00 +0800</pubDate><guid>https://scottlx.github.io/posts/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90cilium-monitor%E6%9C%BA%E5%88%B6/</guid><description>可调试性 报文转发面组件中，可调试性十分关键。开发阶段可能可以使用gdb（ebpf甚至不能用gdb，只能用trace_printk），log等方式进行调试，但到了生产环境，以下几个功能是必须要完备的：
抓包手段
按照网卡抓包 按照流进行抓包 按照特定过滤条件抓包，例如源目的地址，端口，协议号等 报文计数
收发包计数：rx，tx阶段计数 丢包计数：按照错误码进行区分 特定观测点计数：一些重要转发函数，例如l3_fwd, arp_response等 流日志
流量方向：egress/ingress session信息：五元组，nat信息，tcp状态等 其他必要的上下文：例如转发表项查找的结果，构造的action，硬件卸载标记等 linux perf_events ebpf perf基于linux perf_event子系统。epbf通知用户态拷贝数据时基于perf_events的
perf buffer ebpf中提供了内核和用户空间之间高效地交换数据的机制：perf buffer。它是一种per-cpu的环形缓冲区，当我们需要将ebpf收集到的数据发送到用户空间记录或者处理时，就可以用perf buffer来完成。它还有如下特点：
能够记录可变长度数据记； 能够通过内存映射的方式在用户态读取读取数据，而无需通过系统调用陷入到内核去拷贝数据； 实现epoll通知机制 因此在cilium中，实现上述调试手段的思路，就是在转发面代码中构造相应的event到EVENTS_MAP，之后通过别的工具去读取并解析EVENTS_MAP中的数据
EVENTS_MAP定义如下: bpf/lib/events.h
struct { __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY); __uint(key_size, sizeof(__u32)); __uint(value_size, sizeof(__u32)); __uint(pinning, LIBBPF_PIN_BY_NAME); __uint(max_entries, __NR_CPUS__); } EVENTS_MAP __section_maps_btf; key是cpu的编号，因此大小是u32；value一般是文件描述符fd，关联一个perf event，因此也是u32
数据面代码构造好data之后，使用helper function: bpf_perf_event_output通知用户态代码拷贝数据
下面是cilium代码中封装好的event输出函数，最终就是调用的bpf_perf_event_output
// bpf/include/bpf/ctx/skb.h #define ctx_event_output skb_event_output // bpf/include/bpf/helpers_skb.h /* Events for user space */ static int BPF_FUNC_REMAP(skb_event_output, struct __sk_buff *skb, void *map, __u64 index, const void *data, __u32 size) = (void *)BPF_FUNC_perf_event_output; //对应的func id 是 25 // /usr/include/linux/bpf.</description></item><item><title>bpf lpm trie</title><link>https://scottlx.github.io/posts/bpf_lpm_trie/</link><pubDate>Thu, 06 Mar 2025 15:49:00 +0800</pubDate><guid>https://scottlx.github.io/posts/bpf_lpm_trie/</guid><description>lpm有多种实现方式，最常用的是用trie。当然也会有更简单的实现方式，例如某些特定场景用多重哈希表就能解决（ipv4地址，32个掩码对应32个哈希表）
从4.11内核版本开始，bpf map引入了BPF_MAP_TYPE_LPM_TRIE
主要是用于匹配ip地址，内部是将数据存储在一个不平衡的trie中，key使用prefixlen,data
data是以大端网络序存储的，data[0]存的是msb。
prefixlen支持8的整数倍，最高可以是2048。因此除了ip匹配，还可以用来做端口，协议，vpcid等等的扩充匹配。在应用层面上除了做路由表，还可以作为acl，policy等匹配过滤的底层实现
使用方式 BPF_MAP_TYPE_LPM_TRIE — The Linux Kernel documentation
除了上述基本的Ipv4的使用方式，扩展使用方式可以参考一下cillium中IPCACHE_MAP的使用
首先是map的定义
struct ipcache_key { struct bpf_lpm_trie_key lpm_key; __u16 cluster_id; __u8 pad1; __u8 family; union { struct { __u32 ip4; __u32 pad4; __u32 pad5; __u32 pad6; }; union v6addr ip6; }; } __packed; /* Global IP -&amp;gt; Identity map for applying egress label-based policy */ struct { __uint(type, BPF_MAP_TYPE_LPM_TRIE); __type(key, struct ipcache_key); __type(value, struct remote_endpoint_info); __uint(pinning, LIBBPF_PIN_BY_NAME); __uint(max_entries, IPCACHE_MAP_SIZE); __uint(map_flags, BPF_F_NO_PREALLOC); } IPCACHE_MAP __section_maps_btf; 可以看到cillium将v4和v6合并成一个map查询，匹配条件并带上了cluster_id</description></item><item><title>cilium datapath</title><link>https://scottlx.github.io/posts/cilium-datapath/</link><pubDate>Mon, 03 Mar 2025 18:15:00 +0800</pubDate><guid>https://scottlx.github.io/posts/cilium-datapath/</guid><description>hook点 大部分是挂载位置是tc，tc是网络协议栈初始处理挂载点
// linux source code: dev.c __netif_receive_skb_core | list_for_each_entry_rcu(ptype, &amp;amp;ptype_all, list) {...} // packet capture | do_xdp_generic // handle generic xdp | sch_handle_ingress // tc ingress | tcf_classify | __tcf_classify // ebpf program is working here 如果没有下发policy，xdp就不会挂载各类filter程序
网络设备 cillium的网络方案不像常规的网桥模式（ovs，linux bridge），datapath不是一个完整的run to completion，而是分散在各个虚拟接口上，类似pipeline模式
cillium_host: 集群内所有podCIDR的网关，地址对容器可见
cilium_net: cilium_host的veth对，ipvlan模式才会用到？
clilium_vxlan: 用来提供Pod跨节点通信overlay封装
lxcXXXX: 容器veth对在主机侧的接口
同节点pod2pod cillium_host是所有pod的网关，因此会先arp request该地址。arp相应其实是在lxc处被代答了，arp报文不会走到cillium_host
// bpf_lxc.c __section_entry int cil_from_container(struct __ctx_buff *ctx) { ... case bpf_htons(ETH_P_ARP): ret = tail_call_internal(ctx, CILIUM_CALL_ARP, &amp;amp;ext_err); break; .</description></item><item><title>gobpf不完整使用指南</title><link>https://scottlx.github.io/posts/gobpf/</link><pubDate>Mon, 03 Oct 2022 14:00:00 +0800</pubDate><guid>https://scottlx.github.io/posts/gobpf/</guid><description>编译过程 安装llvm-10,clang-10 apt-install llvm-10 clang-10
下载bpf2go go install github.com/cilium/ebpf/cmd/bpf2go@latest 修改bpf程序的include
#include &amp;#34;common.h&amp;#34; 编译时将bpd的headers包含进来 GOPACKAGE=main bpf2go -cc clang-10 -cflags &amp;#39;-O2 -g -Wall -Werror&amp;#39; -target bpfel,bpfeb bpf helloworld.bpf.c -- -I /root/ebpf/examples/headers 得到大端和小端两个版本的ELF文件，之后在go程序里加载即可。cpu一般都是小端。
内核版本要求 经测试一些gobpf的一些syscall不适配较低版本的内核（例如5.8的BPF_LINK_CREATE会报参数错误），建议使用最新版本内核5.19
bpf_map 用户态程序首先加载bpf maps，再将bpf maps绑定到fd上。elf文件中的realocation table用来将代码中的bpf maps重定向至正确的fd上,用户程序在fd上发起bpf syscall
map的value尽量不要存复合数据结构，若bpf程序和用户态程序共用一个头文件，用户态程序调用bpf.Lookup时由于结构体变量unexported而反射失败
pinning object 将map挂载到/sys/fs/bpf
ebpf.CollectionOptions{ Maps: ebpf.MapOptions{ // Pin the map to the BPF filesystem and configure the // library to automatically re-write it in the BPF // program so it can be re-used if it already exists or // create it if not PinPath: pinPath 其他用户态程序获取pinned map的fd</description></item><item><title>初识ebpf</title><link>https://scottlx.github.io/posts/%E5%88%9D%E8%AF%86ebpf/</link><pubDate>Mon, 03 Oct 2022 14:00:00 +0800</pubDate><guid>https://scottlx.github.io/posts/%E5%88%9D%E8%AF%86ebpf/</guid><description>摘自
eBPF 用户空间虚拟机实现相关 | Blog (forsworns.github.io)
[ 译] Cilium：BPF 和 XDP 参考指南（2021） (arthurchiao.art)
hook point 可以插入bpf代码的位置
enum bpf_prog_type { BPF_PROG_TYPE_UNSPEC, BPF_PROG_TYPE_SOCKET_FILTER, BPF_PROG_TYPE_KPROBE, BPF_PROG_TYPE_SCHED_CLS, BPF_PROG_TYPE_SCHED_ACT, BPF_PROG_TYPE_TRACEPOINT, BPF_PROG_TYPE_XDP, BPF_PROG_TYPE_PERF_EVENT, BPF_PROG_TYPE_CGROUP_SKB, BPF_PROG_TYPE_CGROUP_SOCK, BPF_PROG_TYPE_LWT_IN, BPF_PROG_TYPE_LWT_OUT, BPF_PROG_TYPE_LWT_XMIT, BPF_PROG_TYPE_SOCK_OPS, BPF_PROG_TYPE_SK_SKB, }; 程序类型 bpf_prog_type BPF prog 入口参数（R1) 程序类型 BPF_PROG_TYPE_SOCKET_FILTER struct __sk_buff 用于过滤进出口网络报文，功能上和 cBPF 类似。 BPF_PROG_TYPE_KPROBE struct pt_regs 用于 kprobe 功能的 BPF 代码。 BPF_PROG_TYPE_TRACEPOINT 这类 BPF 的参数比较特殊，根据 tracepoint 位置的不同而不同。 用于在各个 tracepoint 节点运行。 BPF_PROG_TYPE_XDP struct xdp_md 用于控制 XDP(eXtreme Data Path)的 BPF 代码。 BPF_PROG_TYPE_PERF_EVENT struct bpf_perf_event_data 用于定义 perf event 发生时回调的 BPF 代码。 BPF_PROG_TYPE_CGROUP_SKB struct __sk_buff 用于在 network cgroup 中运行的 BPF 代码。功能上和 Socket_Filter 近似。具体用法可以参考范例 test_cgrp2_attach。 BPF_PROG_TYPE_CGROUP_SOCK struct bpf_sock 另一个用于在 network cgroup 中运行的 BPF 代码，范例 test_cgrp2_sock2 中就展示了一个利用 BPF 来控制 host 和 netns 间通信的例子。 BPF 程序类型就是由 BPF side 的代码的函数参数确定的，比如写了一个函数，参数是 struct __sk_buff 类型的，它就是一个 BPF_PROG_TYPE_SOCKET_FILTER 类型的 BPF 程序</description></item></channel></rss>