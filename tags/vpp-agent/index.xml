<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>vpp-agent on windseek</title><link>https://scottlx.github.io/tags/vpp-agent/</link><description>Recent content in vpp-agent on windseek</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 07 Apr 2023 15:10:00 +0800</lastBuildDate><atom:link href="https://scottlx.github.io/tags/vpp-agent/index.xml" rel="self" type="application/rss+xml"/><item><title>contiv memif</title><link>https://scottlx.github.io/posts/contiv-memif/</link><pubDate>Fri, 07 Apr 2023 15:10:00 +0800</pubDate><guid>https://scottlx.github.io/posts/contiv-memif/</guid><description>contiv memif contiv的cni与device plugin相结合，实现了：
Pod能同时接入不止一张网卡 Pod接入的网卡可以是tap，veth，memif devicePlugin Device Plugin实际是一个运行在Kubelet所在的Node上的gRPC server，通过Unix Socket、基于以下（简化的）API来和Kubelet的gRPC server通信，并维护对应设备资源在当前Node上的注册、发现、分配、卸载。 其中，ListAndWatch()负责对应设备资源的discovery和watch；Allocate()负责设备资源的分配。
Insight kubelet kubelet接收上图格式的API。API中的annotations定义了pod的网卡个数与类型，resources中定义了所需要的device plugin的资源，也就是memif。
kubelet执行常规的syncPod流程，调用contiv cni创建网络。此时会在请求中将annotation传递给cni。
同时，agent的DevicePluginServer会向kubelet注册rpc服务，注册contivpp.io/memif的设备资源，从而kubelet的device manager会grpc请求DevicePluginServer获取contivpp.io/memif设备资源。
cni cni实现了github.com/containernetworking/cni标准的add和del接口。实际上做的事情只是将cni请求转换为了对agent的grpc请求：解析args，并通过grpc调用agent的接口发送cniRequest，再根据grpc的返回结果，将结果再次转换成标准cni接口的返回格式
Agent podmanager podmanager实现了上述cni调用的grpc server，主要任务是将cni的request转换为内部的event数据格式，供event loop处理。
request是cni定义的请求数据类型，详见https://github.com/containernetworking/cni/blob/master/SPEC.md#parameters
event则是agent内部的关于pod事务模型，类似原生kvScheduler的针对vpp api的transaction。每一种event都会对应一个plugin去实现他的handler，供event loop调用。
event loop event loop是整个contiv agent的核心处理逻辑，北向对接event queue，南向调用各个EventHandler，将event转换为kvScheduler的事务。
执行了以下步骤：
对事件的预处理，包括校验，判断事件类型，加载必要的配置等 判断是否是更新的事件 对事件的handler进行排序，并生成正向或回退的handler顺序 与本次事件无关的handler过滤掉 创建对这次事件的记录record 打印上述步骤生成的所有事件相关信息 执行事件更新或同步，生成vpp-agent里的事务 将contiv生成的配置与外部配置进行merge，得到最终配置 将最终配置的vpp-agent事务commit到agent的kvscheduler 若事务失败，将已经完成的操作进行回退 完成事件，输出记录record与计时 打印回退失败等不可恢复的异常 若开启一致性检查，则最好再执行一次同步校验 devicemanager devicemanager既实现了对接kublet的DevicePluginServer，又实现了AllocateDevice类型的event的handler。换句话说是自己产生并处理自己的event。
主要业务逻辑：
创建memif socket文件的目录并挂载至容器
创建连接socket的secret。
上述的创建并不是真实的创建，而是把需要的信息(event.Envs, event.Annotations, event.Mounts)通过grpc返回给kublet，让kubelet去创建。
devicemanager还会将上述memif的信息保存在缓存中，供其他插件来获取。若缓存中信息不存在，则会调用kubelet的api获取信息。
ipNet ipNet插件主要负责node和pod中各类网卡的创建销毁，vxlan的分配，vrf的分配等
更新网卡时，ipnet会读取annotation中kv，判断网卡类型。若类型为memif，则会向deviceManager获取当前pod里各容器的memifInfo，之后根据memifInfo里的socket地址和secret，创建memif类型的网卡事务，并 push 至kvscheduler</description></item><item><title>基于事务处理的vpp管控面agent</title><link>https://scottlx.github.io/posts/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E7%9A%84vpp%E7%AE%A1%E6%8E%A7%E9%9D%A2agent/</link><pubDate>Mon, 03 Oct 2022 13:00:00 +0800</pubDate><guid>https://scottlx.github.io/posts/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E7%9A%84vpp%E7%AE%A1%E6%8E%A7%E9%9D%A2agent/</guid><description>问题背景 vpp作为vrouter，类似物理交换机，各配置项依赖关系复杂。以下为vpp配置abf策略路由的例子：
typedef abf_policy { u32 policy_id; u32 acl_index; //依赖acl u8 n_paths; vl_api_fib_path_t paths[n_paths]; }; autoreply define abf_policy_add_del { option status=&amp;#34;in_progress&amp;#34;; u32 client_index; u32 context; bool is_add; vl_api_abf_policy_t policy; }; typedef abf_itf_attach { u32 policy_id; vl_api_interface_index_t sw_if_index; //依赖interface，interface又会依赖其他资源 u32 priority; bool is_ipv6; }; 可以看到，策略路由首先依赖acl规则，之后将abf绑定至接口时需要依赖对应interface的index，且创建interface又需要依赖其他资源（绑定vrf等）。
除此之外，vpp配置写入存在中间状态与崩溃的问题，且无法避免。“崩溃”类似数据库写入的概念。数据必须要成功写入磁盘、磁带等持久化存储器后才能拥有持久性，只存储在，内存中的数据，一旦遇到应用程序忽然崩溃，或者数据库、操作系统一侧的崩溃，甚至是机器突然断电宕机等情况就会丢失，这些意外情况都统称为“崩溃”。
因此，为了解决vpp（物理交换机也适用）各配置项的依赖关系，以及保证原子性和持久性，实现崩溃恢复，需要在管控面agent侧处理好上述问题。
事务处理 本人对分布式事务领域涉及不深，以下摘自于
本地事务（也可称为局部事务），是单个服务使用单个数据源场景，也就是最基本的本地数据落盘的事务。本地事务要求底层数据源需要支持事务的开启、终止、提交、回滚、嵌套等。在数据库领域（ARIES理论，基于语义的恢复与隔离），感兴趣的可以研究下commiting logging机制（OceanBase）和shadow paging
全局事务，是单个服务多个数据源场景。主要目的是为了解决事务一致性问题，并做到统一提交，统一回滚的功能。例如我有一个全局事务需要在A表中写入记录a（本地事务A），再在B表中写入记录b（本地事务B），A表和B表分别在两台物理机的磁盘上。在数据存储领域由X/Open XA对此发布了一个事务处理架构，且当前很多分布式事务处理框架都是基于此来设计的。主要核心如下：
全局事务管理器（Transaction Manager，TM）：协调全局事务 局部资源管理器（Resource Manaeger，RM）：驱动本地事务 模型：XA，TCC，SAGA，AT 。。。 感兴趣的可以研究下阿里的seata。
事务处理视角看待vpp管控面 本地事务 vpp的配置是内存上的配置，不需要落盘。 vpp的每个资源的api可视为一个数据源 数据源没有实现事务的开启、终止、提交、回滚、嵌套、设置隔离级别等能力，只提供了下发，删除，读取接口 上述数据源未提供的能力需要agent来补齐 全局事务 agent暴露给上层的接口可视为全局事务 有些全局事务只涉及单个数据源，有些全局事务涉及多个数据源 agent内部需要实现TM，将全局事务转为有序的本地事务列表 agent内部需要实现RM，调用vpp api，驱动本地事务的执行 举例：</description></item></channel></rss>