<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>dpdk on windseek</title><link>https://scottlx.github.io/tags/dpdk/</link><description>Recent content in dpdk on windseek</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 18 Feb 2025 11:11:00 +0800</lastBuildDate><atom:link href="https://scottlx.github.io/tags/dpdk/index.xml" rel="self" type="application/rss+xml"/><item><title>dpvs icmp session</title><link>https://scottlx.github.io/posts/dpvs-icmp/</link><pubDate>Tue, 18 Feb 2025 11:11:00 +0800</pubDate><guid>https://scottlx.github.io/posts/dpvs-icmp/</guid><description>原生的ipvs仅处理三种类型的ICMP报文：ICMP_DEST_UNREACH、ICMP_SOURCE_QUENCH和ICMP_TIME_EXCEEDED
对于不是这三种类型的ICMP，则设置为不相关联(related)的ICMP，返回NF_ACCEPT，之后走本机路由流程
dpvs对ipvs进行了一些修改，修改后逻辑如下
icmp差错报文流程 __dp_vs_in
__dp_vs_in_icmp4 （处理icmp差错报文，入参related表示找到了关联的conn）
若不是ICMP_DEST_UNREACH，ICMP_SOURCE_QUENCH，ICMP_TIME_EXCEEDED，返回到_dp_vs_in走普通conn命中流程
icmp差错报文，需要将报文头偏移到icmp头内部的ip头，根据内部ip头查找内部ip的conn。
若找到conn，表明此ICMP报文是由之前客户端的请求报文所触发的，由真实服务器回复的ICMP报文。将related置1
若未找到则返回accept，返回到_dp_vs_in走普通conn命中流程
​ __xmit_inbound_icmp4 ​ 找net和local路由，之后走__dp_vs_xmit_icmp4
__dp_vs_xmit_icmp4 ​ 数据区的前8个字节恰好覆盖了TCP报文或UDP报文中的端口号字段（前四个字节）
inbound方向根据内部ip的conn修改数据区目的端口为conn-&amp;gt;dport，源端口改为conn-&amp;gt;localport，
outbound方向将目的端口改为conn-&amp;gt;cport，源端口改为conn-&amp;gt;vport
​
client (cport ) &amp;lt;&amp;ndash;&amp;gt; (vport)lb(lport) &amp;lt;&amp;ndash;&amp;gt; rs(dport)
​ 重新计算icmp头的checksum，走ipv4_output
实际应用上的问题
某个rs突然下线，导致有时访问vip轮询到了不可达的rs，rs侧的网关发送了一个dest_unreach的icmp包
该rs的conn还未老化，__dp_vs_in_icmp4流程根据这个icmp的内部差错ip头找到了还未老化的conn，将icmp数据区的port进行修改发回给client
但是一般情况，rs下线后，该rs的conn会老化消失，内层conn未命中，还是走外层icmp的conn命中流程转给client。这样内部数据区的端口信息是错的（dport-&amp;gt;lport，正确情况是vport-&amp;gt;cport）
非差错报文流程 返回_dp_vs_in走普通conn命中流程
原本dp_vs_conn_new流程中，先查找svc。icmp的svc默认使用端口0进行查找。但是ipvsadm命令却对端口0的service添加做了限制，导致无法添加这类svc。
svc = dp_vs_service_lookup(iph-&amp;gt;af, iph-&amp;gt;proto, &amp;amp;iph-&amp;gt;daddr, 0, 0, mbuf, NULL, &amp;amp;outwall, rte_lcore_id()); 若未查到走INET_ACCEPT(也就是继续往下进行走到ipv4_output_fin2查到local路由，若使用dpip addr配上了vip或lip地址，则会触发本地代答)。
若查到svc，则进行conn的schedule，之后会走dp_vs_laddr_bind，但是dp_vs_laddr_bind不支持icmp协议(可以整改)，最终导致svc可以查到但是conn无法建立，最后走INET_DROP。
概括一下：
未命中svc，走后续local route，最终本地代答 命中svc后若conn无法建立，drop 命中svc且建立conn，发往rs或client icmp的conn ​
_ports[0] = icmp4_id(ich); _ports[1] = ich-&amp;gt;type &amp;lt;&amp;lt; 8 | ich-&amp;gt;code; Inbound hash和outboundhash的五元组都使用上述这两个port进行哈希，并与conn进行关联。</description></item><item><title>dpvs route转发</title><link>https://scottlx.github.io/posts/dpvs-route%E8%BD%AC%E5%8F%91/</link><pubDate>Tue, 18 Feb 2025 11:11:00 +0800</pubDate><guid>https://scottlx.github.io/posts/dpvs-route%E8%BD%AC%E5%8F%91/</guid><description>ipv4_rcv_fin 是路由转发逻辑， INET_HOOKPRE_ROUTING 中走完hook逻辑后，根据dpvs返回值走ipv4_rcv_fin
路由表结构体 struct route_entry { uint8_t netmask; short metric; uint32_t flag; unsigned long mtu; struct list_head list; struct in_addr dest; //cf-&amp;gt;dst.in struct in_addr gw;// 下一跳地址，0说明是直连路由，下一跳地址就是报文自己的目的地址，对应配置的cf-&amp;gt;via.in struct in_addr src; // cf-&amp;gt;src.in， 源地址策略路由匹配 struct netif_port *port; // 出接口 rte_atomic32_t refcnt; }; 路由类型 /* dpvs defined. */ #define RTF_FORWARD 0x0400 #define RTF_LOCALIN 0x0800 #define RTF_DEFAULT 0x1000 #define RTF_KNI 0X2000 #define RTF_OUTWALL 0x4000 路由表类型 #define this_route_lcore (RTE_PER_LCORE(route_lcore)) #define this_local_route_table (this_route_lcore.local_route_table) #define this_net_route_table (this_route_lcore.</description></item><item><title>dpvs session 同步</title><link>https://scottlx.github.io/posts/dpvs-session%E5%90%8C%E6%AD%A5/</link><pubDate>Tue, 18 Feb 2025 11:11:00 +0800</pubDate><guid>https://scottlx.github.io/posts/dpvs-session%E5%90%8C%E6%AD%A5/</guid><description>总体架构 lb session同步采用分布式架构，session创建流程触发session数据发送，依次向集群内所有其他节点发送 其他节点收到新的session数据修改本地session表。 session接收和发送各占一个独立线程。
step1: 向所有其他节点发送session数据 remote session：从别的节点同步来的session
local session：本节点收到数据包自己生成的session
step2: session同步至worker 方案一（有锁）： • 独立进程和core处理session同步（per numa） • 每个lcore分配local session和remote session，正常情况下都能直接从local session走掉 • 同步过来的session写到remote session表 • session ip根据fdir走到指定进程
方案二（无锁）： • 独立进程和core处理session同步消息 • 每个lcore 有来local session和remote session，通过owner属性区分。 • 同步过来的session由session_sync core发消息给对应的slave，由对应的slave进行读写，因此可以做到无锁。 • session ip根据fdir走到指定core
session同步具体实现 亟待解决的问题： 同步过来的session什么时候老化？
别的节点上线，本节点要发送哪些session？
别的节点下线，本节点要删除哪些session？
是否要响应下线节点的删除/老化请求？
下线节点怎么知道自己已经下线（数据面）？
解决方案： 方案一： session增加owner属性c owner属性：
conn.owner // indicates who has this session session 同步状态转移图
一条session在一个集群中，应当只有一台机器在使用，所以有一个owner属性，代表这条session被谁拥有，其它所有机器只对这条session的owner发起的增删改查请求做响应。
同步动作 session同步应当时实时的。在以下场景被触发： 新建session 发送方： session新建完成之后：对于tcp，是握手完毕的；对于udp，是第一条连接。 接收方： 接收来自发送方的session，在对应core上新建这条连接，开启老化，老化时间设定为默认时间（1小时）。 fin/rst 发送方： 发送删除session消息 接收方： 接收方接收session，做完校验后在对应core上删除session 老化 发送方： 老化时间超时之后，本地session删除，同时发布老化信息，告知其它lb， 接收方： 其它lb 做完校验后，开始老化这条session。 设备下线 下线后通过控制器更新其他lb的session同步地址信息，不再向该设备同步，同时开始老化全部属于该设备的session。 设备上线（包含设备扩容） 新设备： 新上线设备引流前要接收其他设备的存量session信息，这个功能通过控制器触发完成，控制器感知到新lb上线后通知集群内其他lb向它同步存量session，session数量达到一致时（阿里gw用70%阈值）允许新lb引流。 旧设备： 向目的方发送全部的属于自己的session。</description></item><item><title>dpvs 数据流分析</title><link>https://scottlx.github.io/posts/dpvs%E6%95%B0%E6%8D%AE%E6%B5%81%E5%88%86%E6%9E%90/</link><pubDate>Tue, 18 Feb 2025 11:11:00 +0800</pubDate><guid>https://scottlx.github.io/posts/dpvs%E6%95%B0%E6%8D%AE%E6%B5%81%E5%88%86%E6%9E%90/</guid><description>dpvs ingress流程分析 从 lcore_job_recv_fwd 开始，这个是dpvs收取报文的开始
设备层 dev-&amp;gt;flag &amp;amp; NETIF_PORT_FLAG_FORWARD2KNI &amp;mdash;&amp;gt; 则拷贝一份mbuf到kni队列中，这个由命令行和配置文件决定（做流量镜像，用于抓包）
eth层 netif_rcv_mbuf 这里面涉及到vlan的部分不做过多解析
不支持的协议
目前dpvs支持的协议为ipv4, ipv6, arp。 其它报文类型直接丢给内核。其他类型可以看 eth_types。 to_kni
RTE_ARP_OP_REPLY
复制 nworks-1 份mbuf，发送到其它worker的arp_ring上 ( to_other_worker ), 这份报文fwd到 arp协议.
RTE_ARP_OP_REQUEST
这份报文fwd到 arp协议.
arp协议 arp协议处理 neigh_resolve_input
RTE_ARP_OP_REPLY
建立邻居表，记录信息，并且把这个报文送给内核。 to_kni
RTE_ARP_OP_REQUEST
无条件返回网卡的ip以及mac地址 (free arp), netif_xmit 发送到 core_tx_queue
其它op_code
drop
ip层 ipv4协议 (ipv6数据流程上一致) ipv4_rcv
ETH_PKT_OTHERHOST
报文的dmac不是自己， drop
ipv4 协议校验
不通过， drop
下一层协议为 IPPROTO_OSPF
to_kni
INET_HOOK_PRE_ROUTING hook
hook_list: dp_vs_in , dp_vs_prerouting
这两个都与synproxy有关系，但是我们不会启用这个代理，不过需要注意的是syncproxy不通过时会丢包 drop</description></item><item><title>dpdk rcu lib</title><link>https://scottlx.github.io/posts/dpdk-rcu/</link><pubDate>Fri, 08 Dec 2023 17:48:00 +0800</pubDate><guid>https://scottlx.github.io/posts/dpdk-rcu/</guid><description>linux的RCU主要针对的数据对象是链表，目的是提高遍历读取数据的效率，为了达到目的使用RCU机制读取数据的时候不对链表进行耗时的加锁操作。这样在同一时间可以有多个线程同时读取该链表，并且允许一个线程对链表进行修改。RCU适用于需要频繁的读取数据，而相应修改数据并不多的情景。
dpdk中由于writer和reader同时访问一段内存，删除元素的时候需要确保
删除时不会将内存put回allocator，而是删掉这段内存的引用。这样确保了新的访问者不会拿到这个元素的引用，而老的访问者不会在访问过程中core掉 只有在元素没有任何引用计数时，才释放掉该元素的内存 静默期是指线程没有持有共享内存的引用的时期，也就是下图绿色的时期
上图中，有三个read thread，T1， T2，T3。两条黑色竖线分别代表writer执行delete和free的时刻。
执行delete时，T1和T2还拿着entry1和entry2的reference，此时writer还不能free entry1或entry2的内存，只能删除元素的引用.
writer必须等到执行delete时，当时引用该元素的的线程，都完成了一个静默期之后，才可以free这个内存。
writer不需要等T3进入静默期，因为执行delete时，T3还在静默期。
如何实现RCU机制
writer需要一直轮询reader的状态，看是否进入静默期。这样会导致一直循环轮询，造成额外的cpu消耗。由于需要等reader的静默期结束，reader的静默期越长，reader的数量越多，writer cpu的消耗会越大，因此我们需要短的grace period。但是如果将reader的critical section减小，虽然writer的轮询变快了，但是reader的报告次数增加，reader的cpu消耗会增加，因此我们需要长的critical section。这两者之间看似矛盾。 长的critical section：dpdk的lcore一般都是一个while循环。循环的开始和结束必定是静默期。循环的过程中肯定是在访问各种各样的共享内存。因此critical section的粒度可以不要很细，不要每次访问的时候退出静默期，不访问的时候进入静默期，而是将整个循环认为是critical section，只有在循环的开始退出静默期，循环的结束进入静默期。 短的grace period：如果是pipeline模型，并不是所有worker都会使用相同的数据结构。话句话说，同一个元素，只会被部分的worker所引用和读取。因此writer不需要等到所有worker的critical section结束，而是使用该元素的worker结束critical section。这样将grace period粒度变小之后，缩短了writer整体的grace period。这种粒度的控制是通过 qsbr 实现的 如何使用rcu库 dpdk-stable-20.11.1/app/test/test_rcu_qsbr.c test_rcu_qsbr_sw_sv_3qs
先创建出struct rte_rcu_qsbr
sz = rte_rcu_qsbr_get_memsize(RTE_MAX_LCORE); rv = (struct rte_rcu_qsbr *)rte_zmalloc(NULL, sz, RTE_CACHE_LINE_SIZE); 再初始化QS variable
rte_rcu_qsbr_init(rv, RTE_MAX_LCORE); Reader注册自己的线程号，并上线（将自己加到writer的轮询队列里面） online时会原子读qsbr里的token，并设置到v-&amp;gt;qsbr_cnt[thread_id].cnt中
(void)rte_rcu_qsbr_thread_register(rv, lcore_id); rte_rcu_qsbr_thread_online(rv, lcore_id); 每次读取共享数据后，更新自己的静默状态（rte_rcu_qsbr_quiescent）
do { for (i = 0; i &amp;lt; num_keys; i += j) { for (j = 0; j &amp;lt; QSBR_REPORTING_INTERVAL; j++) rte_hash_lookup(tbl_rwc_test_param.</description></item></channel></rss>