<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>etcd on windseek</title><link>https://scottlx.github.io/tags/etcd/</link><description>Recent content in etcd on windseek</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 28 Oct 2022 09:30:00 +0800</lastBuildDate><atom:link href="https://scottlx.github.io/tags/etcd/index.xml" rel="self" type="application/rss+xml"/><item><title>raft选举流程</title><link>https://scottlx.github.io/posts/raft%E9%80%89%E4%B8%BE%E6%B5%81%E7%A8%8B/</link><pubDate>Fri, 28 Oct 2022 09:30:00 +0800</pubDate><guid>https://scottlx.github.io/posts/raft%E9%80%89%E4%B8%BE%E6%B5%81%E7%A8%8B/</guid><description>图解 Raft (thesecretlivesofdata.com)
算法目的：实现了分布式节点的数据一致性
节点有三个状态：follower，candidate，leader
leader election 初始阶段所有节点处于follower状态
follower状态下节点存在一个election timeout（150ms—300ms之间的随机数，随机降低了多个节点同时升级为candidate的可能性），election timeout内没有收到leader的heartbeat后，会自动升级为candidate状态，并开始一个新的election term。term是全局的，表示整个集群发生过选举的轮次(任期)。
candidate状态下，节点会向集群内所有节点发送requests votes请求。其他节点收到requests votes请求后，如果在本次term内还没有投过票，则会返回选票，如果candidate收到的选票占集群节点的大多数，则升级为本次term的leader节点。升级为leader之后向他的follower 发送append entries消息（也就是包含entry消息的心跳），follower也会返回消息的response，系统正常情况下维持在该状态
如果选举时，在一个term内发生了两个节点有同样的选票，会在超时过后进入下一轮进行重新选举
log replication client的请求只会发往leader。leader收到改动后，将改动写入日志（还未持久化commit），并将改动通过heartbeat广播至follower节点。follower节点写了entry之后（此时还未commit），返回ack。leader收到大于集群节点一半的ack之后，认为已经可以commit了，广播commit的通知。最终集群内所有follower触发commit，向leader返回ack。最后leader认为集群已经达成一致性了，向client返回ack
如果集群中产生网络隔离，每个隔离域中会产生一个新的leader，整个集群会存在多个leader。follower少的leader由于获取不到majority ack，他的entry不会被commit。此时client往另一个follower多的leader发送数据改变请求，该隔离域的节点会被commit
此时去掉网络隔离后，之前follower少的隔离域内未commit的entry会被刷成之前follower多的隔离域的entry,随后commit，此时集群再次达成一致性</description></item><item><title>etcd client v3 连接流程</title><link>https://scottlx.github.io/posts/etcd-client-v3%E8%BF%9E%E6%8E%A5%E6%B5%81%E7%A8%8B/</link><pubDate>Fri, 28 Oct 2022 09:15:00 +0800</pubDate><guid>https://scottlx.github.io/posts/etcd-client-v3%E8%BF%9E%E6%8E%A5%E6%B5%81%E7%A8%8B/</guid><description>首先需要了解grpc框架的一些概念，这边引用网上的一张图 Resolver 提供一个用户自定义的解析、修改地址的方法，使得用户可以自己去实现地址解析的逻辑、做服务发现、地址更新等等功能。
将Endpoints里的ETCD服务器地址(127.0.0.1:2379这种格式)做一次转换传给grpc框架。也可以自己重新写此resolver，做服务发现功能。例如etcd服务器地址写nacos之类的地址，在resolver中写好转换逻辑。 调用ClientConn的ParseServiceConfig接口告诉endpoints的负载策略是轮询 Balancer 管理subConns，并收集各conn信息，更新状态至ClientConn 生成picker(balancer)的快照，从而ClientConn可以选择发送rpc请求的subConn 此处etcd client没有实现balancer，默认使用grpc提供的轮询的balancer
重试策略 与一般的c-s模型不同，etcd client的重试是针对集群的重试。单个节点的断连不会造成所有节点的重连。
重试机制 一般的重试是对同一个节点进行重试，但etcd client的自动重试不会在ETCD集群的同一节点上进行，是轮询重试集群的每个节点。重试时不会重新建连，而是使用balancer提供的transport。transport的状态更新与这一块的重试是通过balancer解耦的。
重试条件 etcd unary拦截器 拦截器类似http里的中间件的概念，在发送实际请求之前对报文进行篡改。一般用来添加认证，日志记录，缓存之类的功能。
此处etcd的一元拦截器主要做了自动重试的功能，且只会重试一些特定的错误(DeadlineExceeded, Canceled,ErrInvalidAuthToken)
func (c *Client) unaryClientInterceptor(optFuncs ...retryOption) grpc.UnaryClientInterceptor { ... if isContextError(lastErr) { if ctx.Err() != nil { // its the context deadline or cancellation. return lastErr } // its the callCtx deadline or cancellation, in which case try again. continue } if callOpts.retryAuth &amp;amp;&amp;amp; rpctypes.Error(lastErr) == rpctypes.ErrInvalidAuthToken { // clear auth token before refreshing it.</description></item></channel></rss>