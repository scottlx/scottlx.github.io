<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Vpp on windseek</title><link>https://scottlx.github.io/tags/vpp/</link><description>Recent content in Vpp on windseek</description><generator>Hugo</generator><language>zh-CN</language><lastBuildDate>Fri, 07 Apr 2023 15:10:00 +0800</lastBuildDate><atom:link href="https://scottlx.github.io/tags/vpp/index.xml" rel="self" type="application/rss+xml"/><item><title>contiv memif</title><link>https://scottlx.github.io/posts/contiv-memif/</link><pubDate>Fri, 07 Apr 2023 15:10:00 +0800</pubDate><guid>https://scottlx.github.io/posts/contiv-memif/</guid><description>&lt;h3 id="contiv-memif">contiv memif&lt;/h3>
&lt;p>contiv的cni与device plugin相结合，实现了：&lt;/p>
&lt;ol>
&lt;li>Pod能同时接入不止一张网卡&lt;/li>
&lt;li>Pod接入的网卡可以是tap，veth，memif&lt;/li>
&lt;/ol>
&lt;h4 id="deviceplugin">devicePlugin&lt;/h4>
&lt;p>Device Plugin实际是一个运行在Kubelet所在的Node上的gRPC server，通过Unix Socket、基于以下（简化的）API来和Kubelet的gRPC server通信，并维护对应设备资源在当前Node上的注册、发现、分配、卸载。
其中，&lt;code>ListAndWatch()&lt;/code>负责对应设备资源的discovery和watch；&lt;code>Allocate()&lt;/code>负责设备资源的分配。&lt;/p>
&lt;p>&lt;img src="https://scottlx.github.io/img/vpp-agent/6F9684EB-7E5E-4b77-9316-32D5C92FD07E.png" alt="6F9684EB-7E5E-4b77-9316-32D5C92FD07E">&lt;/p>
&lt;h4 id="insight">Insight&lt;/h4>
&lt;p>&lt;img src="https://scottlx.github.io/img/vpp-agent/contiveCNI.drawio.png" alt="contiveCNI.drawio">&lt;/p>
&lt;h5 id="kubelet">kubelet&lt;/h5>
&lt;p>kubelet接收上图格式的API。API中的annotations定义了pod的网卡个数与类型，resources中定义了所需要的device plugin的资源，也就是memif。&lt;/p>
&lt;p>kubelet执行常规的syncPod流程，调用contiv cni创建网络。此时会在请求中将annotation传递给cni。&lt;/p>
&lt;p>同时，agent的DevicePluginServer会向kubelet注册rpc服务，注册contivpp.io/memif的设备资源，从而kubelet的device manager会grpc请求DevicePluginServer获取contivpp.io/memif设备资源。&lt;/p>
&lt;h5 id="cni">cni&lt;/h5>
&lt;p>cni实现了github.com/containernetworking/cni标准的add和del接口。实际上做的事情只是将cni请求转换为了对agent的grpc请求：解析args，并通过grpc调用agent的接口发送cniRequest，再根据grpc的返回结果，将结果再次转换成标准cni接口的返回格式&lt;/p>
&lt;h5 id="agent">Agent&lt;/h5>
&lt;h6 id="podmanager">podmanager&lt;/h6>
&lt;p>podmanager实现了上述cni调用的grpc server，主要任务是将cni的request转换为内部的event数据格式，供event loop处理。&lt;/p>
&lt;p>request是cni定义的请求数据类型，详见https://github.com/containernetworking/cni/blob/master/SPEC.md#parameters&lt;/p>
&lt;p>event则是agent内部的关于pod事务模型，类似原生kvScheduler的针对vpp api的transaction。每一种event都会对应一个plugin去实现他的handler，供event loop调用。&lt;/p>
&lt;h6 id="event-loop">event loop&lt;/h6>
&lt;p>event loop是整个contiv agent的核心处理逻辑，北向对接event queue，南向调用各个EventHandler，将event转换为kvScheduler的事务。&lt;/p>
&lt;p>执行了以下步骤：&lt;/p>
&lt;ol>
&lt;li>对事件的预处理，包括校验，判断事件类型，加载必要的配置等&lt;/li>
&lt;li>判断是否是更新的事件&lt;/li>
&lt;li>对事件的handler进行排序，并生成正向或回退的handler顺序&lt;/li>
&lt;li>与本次事件无关的handler过滤掉&lt;/li>
&lt;li>创建对这次事件的记录record&lt;/li>
&lt;li>打印上述步骤生成的所有事件相关信息&lt;/li>
&lt;li>执行事件更新或同步，生成vpp-agent里的事务&lt;/li>
&lt;li>将contiv生成的配置与外部配置进行merge，得到最终配置&lt;/li>
&lt;li>将最终配置的vpp-agent事务commit到agent的kvscheduler&lt;/li>
&lt;li>若事务失败，将已经完成的操作进行回退&lt;/li>
&lt;li>完成事件，输出记录record与计时&lt;/li>
&lt;li>打印回退失败等不可恢复的异常&lt;/li>
&lt;li>若开启一致性检查，则最好再执行一次同步校验&lt;/li>
&lt;/ol>
&lt;h6 id="devicemanager">devicemanager&lt;/h6>
&lt;p>devicemanager既实现了对接kublet的DevicePluginServer，又实现了AllocateDevice类型的event的handler。换句话说是自己产生并处理自己的event。&lt;/p>
&lt;p>主要业务逻辑：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>创建memif socket文件的目录并挂载至容器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建连接socket的secret。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>上述的创建并不是真实的创建，而是把需要的信息(event.Envs, event.Annotations, event.Mounts)通过grpc返回给kublet，让kubelet去创建。&lt;/p>
&lt;p>devicemanager还会将上述memif的信息保存在缓存中，供其他插件来获取。若缓存中信息不存在，则会调用kubelet的api获取信息。&lt;/p>
&lt;h6 id="ipnet">ipNet&lt;/h6>
&lt;p>ipNet插件主要负责node和pod中各类网卡的创建销毁，vxlan的分配，vrf的分配等&lt;/p>
&lt;p>更新网卡时，ipnet会读取annotation中kv，判断网卡类型。若类型为memif，则会向deviceManager获取当前pod里各容器的memifInfo，之后根据memifInfo里的socket地址和secret，创建memif类型的网卡事务，并 push 至kvscheduler&lt;/p></description></item><item><title>基于事务处理的vpp管控面agent</title><link>https://scottlx.github.io/posts/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E7%9A%84vpp%E7%AE%A1%E6%8E%A7%E9%9D%A2agent/</link><pubDate>Mon, 03 Oct 2022 13:00:00 +0800</pubDate><guid>https://scottlx.github.io/posts/%E5%9F%BA%E4%BA%8E%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E7%9A%84vpp%E7%AE%A1%E6%8E%A7%E9%9D%A2agent/</guid><description>&lt;h3 id="问题背景">问题背景&lt;/h3>
&lt;p>vpp作为vrouter，类似物理交换机，各配置项依赖关系复杂。以下为vpp配置abf策略路由的例子：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#719e07">typedef&lt;/span> abf_policy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u32 policy_id;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u32 acl_index; &lt;span style="color:#586e75">//依赖acl
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#586e75">&lt;/span> u8 n_paths;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">vl_api_fib_path_t&lt;/span> paths[n_paths];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>autoreply define abf_policy_add_del
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> option status&lt;span style="color:#719e07">=&lt;/span>&lt;span style="color:#2aa198">&amp;#34;in_progress&amp;#34;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u32 client_index;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u32 context;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">bool&lt;/span> is_add;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">vl_api_abf_policy_t&lt;/span> policy;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#719e07">typedef&lt;/span> abf_itf_attach
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u32 policy_id;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">vl_api_interface_index_t&lt;/span> sw_if_index; &lt;span style="color:#586e75">//依赖interface，interface又会依赖其他资源
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#586e75">&lt;/span> u32 priority;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">bool&lt;/span> is_ipv6;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>};
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以看到，策略路由首先依赖acl规则，之后将abf绑定至接口时需要依赖对应interface的index，且创建interface又需要依赖其他资源（绑定vrf等）。&lt;/p>
&lt;p>除此之外，vpp配置写入存在中间状态与崩溃的问题，且无法避免。“崩溃”类似数据库写入的概念。数据必须要成功写入磁盘、磁带等持久化存储器后才能拥有持久性，只存储在，内存中的数据，一旦遇到应用程序忽然崩溃，或者数据库、操作系统一侧的崩溃，甚至是机器突然断电宕机等情况就会丢失，这些意外情况都统称为“崩溃”。&lt;/p>
&lt;p>因此，为了解决vpp（物理交换机也适用）各配置项的依赖关系，以及保证原子性和持久性，实现崩溃恢复，需要在管控面agent侧处理好上述问题。&lt;/p>
&lt;h3 id="事务处理">事务处理&lt;/h3>
&lt;p>本人对分布式事务领域涉及不深，以下摘自于&lt;/p>
&lt;p>本地事务（也可称为局部事务），是单个服务使用单个数据源场景，也就是最基本的本地数据落盘的事务。本地事务要求底层数据源需要支持事务的开启、终止、提交、回滚、嵌套等。在数据库领域（ARIES理论，基于语义的恢复与隔离），感兴趣的可以研究下commiting logging机制（OceanBase）和shadow paging&lt;/p>
&lt;p>全局事务，是单个服务多个数据源场景。主要目的是为了解决事务一致性问题，并做到统一提交，统一回滚的功能。例如我有一个全局事务需要在A表中写入记录a（本地事务A），再在B表中写入记录b（本地事务B），A表和B表分别在两台物理机的磁盘上。在数据存储领域由X/Open XA对此发布了一个事务处理架构，且当前很多分布式事务处理框架都是基于此来设计的。主要核心如下：&lt;/p>
&lt;ul>
&lt;li>全局事务管理器（Transaction Manager，TM）：协调全局事务&lt;/li>
&lt;li>局部资源管理器（Resource Manaeger，RM）：驱动本地事务&lt;/li>
&lt;li>模型：XA，TCC，SAGA，AT 。。。&lt;/li>
&lt;/ul>
&lt;p>感兴趣的可以研究下阿里的seata。&lt;/p>
&lt;h3 id="事务处理视角看待vpp管控面">事务处理视角看待vpp管控面&lt;/h3>
&lt;h4 id="本地事务">本地事务&lt;/h4>
&lt;ul>
&lt;li>vpp的配置是内存上的配置，不需要落盘。&lt;/li>
&lt;li>vpp的每个资源的api可视为一个数据源&lt;/li>
&lt;li>数据源没有实现事务的开启、终止、提交、回滚、嵌套、设置隔离级别等能力，只提供了下发，删除，读取接口&lt;/li>
&lt;li>上述数据源未提供的能力需要agent来补齐&lt;/li>
&lt;/ul>
&lt;h4 id="全局事务">全局事务&lt;/h4>
&lt;ul>
&lt;li>agent暴露给上层的接口可视为全局事务&lt;/li>
&lt;li>有些全局事务只涉及单个数据源，有些全局事务涉及多个数据源&lt;/li>
&lt;li>agent内部需要实现TM，将全局事务转为有序的本地事务列表&lt;/li>
&lt;li>agent内部需要实现RM，调用vpp api，驱动本地事务的执行&lt;/li>
&lt;/ul>
&lt;p>举例：&lt;/p></description></item><item><title>初识srv6</title><link>https://scottlx.github.io/posts/%E5%88%9D%E8%AF%86srv6/</link><pubDate>Sat, 01 Oct 2022 09:18:21 +0800</pubDate><guid>https://scottlx.github.io/posts/%E5%88%9D%E8%AF%86srv6/</guid><description>&lt;p>翻译自SRv6 Network Programming
draft-filsfils-spring-srv6-network-programming-07&lt;/p>
&lt;h3 id="srh">SRH&lt;/h3>
&lt;p>Segment Routing Header&lt;/p>
&lt;p>SRH在一个报文中可以有多个&lt;/p>
&lt;h3 id="nh">NH&lt;/h3>
&lt;p>ipv6 next-header field&lt;/p>
&lt;p>Srv6的Routing Header的type是4，IP6 header的NH字段是43&lt;/p>
&lt;h3 id="sid">SID&lt;/h3>
&lt;p>编排链节点的ID，srv6节点的SID table里面保存自己在各个编排链内的SID。local SID可以是设备外部接口（不会是内部接口）的ipv6地址。例如已经在外部接口配置了地址A和地址B，内部loopback配置了地址C。地址A和地址B会默认被加入到SID Table。&lt;/p>
&lt;p>地址B可以是路由不可达的，为什么？&lt;/p>
&lt;p>可以将地址A理解成全局segments，地址B为本地segments。只要报文在发送时加入了SID list&amp;lt;A,B&amp;gt;，A在B的前面，只要A对外路由可达，报文就会被送到A，然后在本地进行下一步的处理（发往本地的B）&lt;/p>
&lt;p>(SA,DA) (S3, S2, S1; SL)&lt;/p>
&lt;p>S1是第一跳，S3是最后一跳。SL剩下几跳，也可理解为下一个SID节点的下标。例如SL=0， 表示SRH[0]=S3，下一个SID处理节点的ip地址是S3&lt;/p>
&lt;h3 id="sid格式">SID格式&lt;/h3>
&lt;p>SID Table中并不是以Ip的形式保存SID的&lt;/p>
&lt;p>LOC:FUNCT:ARGS::&lt;/p>
&lt;h3 id="function">function&lt;/h3>
&lt;p>每个SID可以绑定多个function。function与SID的绑定关系存在SID Table中。这个特性决定了SRV6的高度可编程性。&lt;/p>
&lt;p>function太多，不一一列出，总结下规律&lt;/p>
&lt;p>带有D的，表示Decapsulation，如果SL==0（已经是最后一跳）且NH!=SRH(没有嵌套另一个SRH)，且SRH的ENH（下一层header类别）符合function的定义(例如DT6，ENH必须是41(ipv6 encapsulation))，则剥去SRH&lt;/p>
&lt;p>带有T的，表示table，查对应的fib表&lt;/p>
&lt;p>带有X的，表示cross-connect，往邻接表对应的Ip地址发（直接拿mac）&lt;/p>
&lt;p>带V的，表示Vlan，往对应Vlan发（改Vlan头部）&lt;/p>
&lt;p>带B的，表示bond，insert在老的SRH和Ipv6 Header之间新插入一个SRH，将DA改为新的SRH的第一个segment；encap则是在最外面新插入一个ipv6头部，新ipv6头部SA是内部ipv6头部的SA，DA是新ipv6头部下的SRH的第一跳&lt;/p></description></item></channel></rss>